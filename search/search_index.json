{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"assembly/","text":"De-novo Genome Assembly # Lecture # Practical # In this practical we will perform the assembly of M. genitalium , a bacterium published in 1995 by Fraser et al in Science ( abstract link ). Getting the data # M. genitalium was sequenced using the MiSeq platform (2 * 150bp). The reads were deposited in the ENA Short Read Archive under the accession ERR486840 Download the 2 fastq files associated with the run. wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_1.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_2.fastq.gz The files that were deposited in ENA were already trimmed, so we do not have to trim ourselves! Question How many reads are in the files? De-novo assembly # We will be using the MEGAHIT assembler to assemble our bacterium megahit -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz -o m_genitalium This will take a few minutes. The result of the assembly is in the directory m_genitalium under the name final.contigs.fa Let's make a copy of it cp m_genitalium/final.contigs.fa m_genitalium.fasta and look at it head m_genitalium.fasta Quality of the Assembly # QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including Run Quast on your assembly quast.py m_genitalium.fasta -o m_genitalium_report and take a look at the text report cat m_genitalium_report/report.txt You should see something like All statistics are based on contigs of size >= 500 bp, unless otherwise noted (e.g., \"# contigs (>= 0 bp)\" and \"Total length (>= 0 bp)\" include all contigs). Assembly m_genitalium # contigs (>= 0 bp) 17 # contigs (>= 1000 bp) 8 # contigs (>= 5000 bp) 7 # contigs (>= 10000 bp) 6 # contigs (>= 25000 bp) 5 # contigs (>= 50000 bp) 2 Total length (>= 0 bp) 584267 Total length (>= 1000 bp) 580160 Total length (>= 5000 bp) 577000 Total length (>= 10000 bp) 570240 Total length (>= 25000 bp) 554043 Total length (>= 50000 bp) 446481 # contigs 11 Largest contig 368542 Total length 582257 GC (%) 31.71 N50 368542 N75 77939 L50 1 L75 2 # N's per 100 kbp 0.00 which is a summary stats about our assembly. Additionally, the file m_genitalium_report/report.html You can either download it and open it in your own web browser, or we make it available for your convenience: m_genitalium_report/report.html Note N50: length for which the collection of all contigs of that length or longer covers at least 50% of assembly length Question How well does the assembly total consensus size and coverage correspond to your earlier estimation? Question How many contigs in total did the assembly produce? Question What is the N50 of the assembly? What does this mean? Fixing misassemblies # Pilon is a software tool which can be used to automatically improve draft assemblies. It attempts to make improvements to the input genome, including: Single base differences Small Indels Larger Indels or block substitution events Gap filling Identification of local misassemblies, including optional opening of new gaps Pilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome. Before running Pilon itself, we have to align our reads against the assembly bowtie2-build m_genitalium.fasta m_genitalium bowtie2 -x m_genitalium -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz | \\ samtools view -bS -o m_genitalium.bam samtools sort m_genitalium.bam -o m_genitalium.sorted.bam samtools index m_genitalium.sorted.bam then we run Pilon pilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved which will correct eventual mismatches in our assembly and write the new improved assembly to m_genitalium_improved.fasta Assembly Completeness # Although quast output a range of metric to assess how contiguous our assembly is, having a long N50 does not guarantee a good assembly: it could be riddled by misassemblies! We will run busco to try to find marker genes in our assembly. Marker genes are conserved across a range of species and finding intact conserved genes in our assembly would be a good indication of its quality First we need to download and unpack the bacterial datasets used by busco wget http://busco.ezlab.org/v2/datasets/bacteria_odb9.tar.gz tar xzf bacteria_odb9.tar.gz then we can run busco with BUSCO.py -i m_genitalium.fasta -l bacteria_odb9 -o busco_genitalium -m genome Question How many marker genes has busco found? Course literature # Course litteraturer for today is: Next-Generation Sequence Assembly: Four Stages of Data Processing and Computational Challenges: https://doi.org/10.1371/journal.pcbi.1003345","title":"De-novo Genome Assembly"},{"location":"assembly/#de-novo-genome-assembly","text":"","title":"De-novo Genome Assembly"},{"location":"assembly/#lecture","text":"","title":"Lecture"},{"location":"assembly/#practical","text":"In this practical we will perform the assembly of M. genitalium , a bacterium published in 1995 by Fraser et al in Science ( abstract link ).","title":"Practical"},{"location":"assembly/#getting-the-data","text":"M. genitalium was sequenced using the MiSeq platform (2 * 150bp). The reads were deposited in the ENA Short Read Archive under the accession ERR486840 Download the 2 fastq files associated with the run. wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_1.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_2.fastq.gz The files that were deposited in ENA were already trimmed, so we do not have to trim ourselves! Question How many reads are in the files?","title":"Getting the data"},{"location":"assembly/#de-novo-assembly","text":"We will be using the MEGAHIT assembler to assemble our bacterium megahit -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz -o m_genitalium This will take a few minutes. The result of the assembly is in the directory m_genitalium under the name final.contigs.fa Let's make a copy of it cp m_genitalium/final.contigs.fa m_genitalium.fasta and look at it head m_genitalium.fasta","title":"De-novo assembly"},{"location":"assembly/#quality-of-the-assembly","text":"QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including Run Quast on your assembly quast.py m_genitalium.fasta -o m_genitalium_report and take a look at the text report cat m_genitalium_report/report.txt You should see something like All statistics are based on contigs of size >= 500 bp, unless otherwise noted (e.g., \"# contigs (>= 0 bp)\" and \"Total length (>= 0 bp)\" include all contigs). Assembly m_genitalium # contigs (>= 0 bp) 17 # contigs (>= 1000 bp) 8 # contigs (>= 5000 bp) 7 # contigs (>= 10000 bp) 6 # contigs (>= 25000 bp) 5 # contigs (>= 50000 bp) 2 Total length (>= 0 bp) 584267 Total length (>= 1000 bp) 580160 Total length (>= 5000 bp) 577000 Total length (>= 10000 bp) 570240 Total length (>= 25000 bp) 554043 Total length (>= 50000 bp) 446481 # contigs 11 Largest contig 368542 Total length 582257 GC (%) 31.71 N50 368542 N75 77939 L50 1 L75 2 # N's per 100 kbp 0.00 which is a summary stats about our assembly. Additionally, the file m_genitalium_report/report.html You can either download it and open it in your own web browser, or we make it available for your convenience: m_genitalium_report/report.html Note N50: length for which the collection of all contigs of that length or longer covers at least 50% of assembly length Question How well does the assembly total consensus size and coverage correspond to your earlier estimation? Question How many contigs in total did the assembly produce? Question What is the N50 of the assembly? What does this mean?","title":"Quality of the Assembly"},{"location":"assembly/#fixing-misassemblies","text":"Pilon is a software tool which can be used to automatically improve draft assemblies. It attempts to make improvements to the input genome, including: Single base differences Small Indels Larger Indels or block substitution events Gap filling Identification of local misassemblies, including optional opening of new gaps Pilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome. Before running Pilon itself, we have to align our reads against the assembly bowtie2-build m_genitalium.fasta m_genitalium bowtie2 -x m_genitalium -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz | \\ samtools view -bS -o m_genitalium.bam samtools sort m_genitalium.bam -o m_genitalium.sorted.bam samtools index m_genitalium.sorted.bam then we run Pilon pilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved which will correct eventual mismatches in our assembly and write the new improved assembly to m_genitalium_improved.fasta","title":"Fixing misassemblies"},{"location":"assembly/#assembly-completeness","text":"Although quast output a range of metric to assess how contiguous our assembly is, having a long N50 does not guarantee a good assembly: it could be riddled by misassemblies! We will run busco to try to find marker genes in our assembly. Marker genes are conserved across a range of species and finding intact conserved genes in our assembly would be a good indication of its quality First we need to download and unpack the bacterial datasets used by busco wget http://busco.ezlab.org/v2/datasets/bacteria_odb9.tar.gz tar xzf bacteria_odb9.tar.gz then we can run busco with BUSCO.py -i m_genitalium.fasta -l bacteria_odb9 -o busco_genitalium -m genome Question How many marker genes has busco found?","title":"Assembly Completeness"},{"location":"assembly/#course-literature","text":"Course litteraturer for today is: Next-Generation Sequence Assembly: Four Stages of Data Processing and Computational Challenges: https://doi.org/10.1371/journal.pcbi.1003345","title":"Course literature"},{"location":"basecalling/","text":"BASE CALLING # objective # Base calling is the process of translating the electronic raw signal (fast5 format) of the sequencer into bases (fastq format). environment # This process will be carried out on the i-Trop computing cluster. software # guppy QUESTION 1: What is the lasted version of guppy installed on the i-Trop computing cluster? location of input files and how to enter them in the command line # long_reads_analyses/fast5/ -i [ --input_path ] Path to input files. location of output files and how to set it in the command line # long_reads_analyses/guppy_output/ -s [ --save_path ] Path to save output files. other required files and how to set them in the command line # Entering information on which flowcell and library preparation kit was used for sequencing is mandatory for launching guppy. This data is found in configuration files. Each library preparation kit/flowcell pair corresponds to a unique configuration file. -c [ --config ] Configuration file for application. To list available configuration files enter guppy_basecaller \u2013\u2013print_workflows (to be entered manually!). The provided data was prepared using the SQK-LSK108 library preparation kit that was sequenced on a MIN106 flowcell. QUESTION 2: What is the name of the configuration file guppy needs to basecall the tutorial data? command line to launch guppy # guppy_basecaller \\ -i */fast5 \\ -s */guppy_out \\ -c dna_*.cfg \\ --num_callers 1 --cpu_threads_per_caller 1 Let the run continue and go to the directory containing precompiled output files. It contains pass and fail directories, each containing fastq files. Check the sequencing_summary file. QUESTION 3: What is the minimal threshold for the mean quality score (mean qscore) to pass quality evaluation? concatenate fastq files passing quality evaluation # guppy output is often composed of several fastq files for a single sample that must be concatenated. cat *.fastq > sample1.fastq","title":"Long reads Basecalling"},{"location":"basecalling/#base-calling","text":"","title":"BASE CALLING"},{"location":"basecalling/#objective","text":"Base calling is the process of translating the electronic raw signal (fast5 format) of the sequencer into bases (fastq format).","title":"objective"},{"location":"basecalling/#environment","text":"This process will be carried out on the i-Trop computing cluster.","title":"environment"},{"location":"basecalling/#software","text":"guppy QUESTION 1: What is the lasted version of guppy installed on the i-Trop computing cluster?","title":"software"},{"location":"basecalling/#location-of-input-files-and-how-to-enter-them-in-the-command-line","text":"long_reads_analyses/fast5/ -i [ --input_path ] Path to input files.","title":"location of input files and how to enter them in the command line"},{"location":"basecalling/#location-of-output-files-and-how-to-set-it-in-the-command-line","text":"long_reads_analyses/guppy_output/ -s [ --save_path ] Path to save output files.","title":"location of output files and how to set it in the command line"},{"location":"basecalling/#other-required-files-and-how-to-set-them-in-the-command-line","text":"Entering information on which flowcell and library preparation kit was used for sequencing is mandatory for launching guppy. This data is found in configuration files. Each library preparation kit/flowcell pair corresponds to a unique configuration file. -c [ --config ] Configuration file for application. To list available configuration files enter guppy_basecaller \u2013\u2013print_workflows (to be entered manually!). The provided data was prepared using the SQK-LSK108 library preparation kit that was sequenced on a MIN106 flowcell. QUESTION 2: What is the name of the configuration file guppy needs to basecall the tutorial data?","title":"other required files and how to set them in the command line"},{"location":"basecalling/#command-line-to-launch-guppy","text":"guppy_basecaller \\ -i */fast5 \\ -s */guppy_out \\ -c dna_*.cfg \\ --num_callers 1 --cpu_threads_per_caller 1 Let the run continue and go to the directory containing precompiled output files. It contains pass and fail directories, each containing fastq files. Check the sequencing_summary file. QUESTION 3: What is the minimal threshold for the mean quality score (mean qscore) to pass quality evaluation?","title":"command line to launch guppy"},{"location":"basecalling/#concatenate-fastq-files-passing-quality-evaluation","text":"guppy output is often composed of several fastq files for a single sample that must be concatenated. cat *.fastq > sample1.fastq","title":"concatenate fastq files passing quality evaluation"},{"location":"file_formats/","text":"File Formats # This lecture is aimed at making you discover the most popular file formats used in bioinformatics. You're expected to have basic working knowledge of Linux to be able to follow the lesson. Table of Contents # The fasta format The fastq format The sam/bam format The vcf format The gff format The fasta format # The fasta format was invented in 1988 and designed to represent nucleotide or peptide sequences. It originates from the FASTA software package, but is now a standard in the world of bioinformatics. The first line in a FASTA file starts with a \">\" (greater-than) symbol followed by the description or identifier of the sequence. Following the initial line (used for a unique description of the sequence) is the actual sequence itself in standard one-letter code. A few sample sequences: >KX580312.1 Homo sapiens truncated breast cancer 1 (BRCA1) gene, exon 15 and partial cds GTCATCCCCTTCTAAATGCCCATCATTAGATGATAGGTGGTACATGCACAGTTGCTCTGGGAGTCTTCAG AATAGAAACTACCCATCTCAAGAGGAGCTCATTAAGGTTGTTGATGTGGAGGAGTAACAGCTGGAAGAGT CTGGGCCACACGATTTGACGGAAACATCTTACTTGCCAAGGCAAGATCTAG >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE A fasta file can contain multiple sequence. Each sequence will be separated by their \"header\" line, starting by \">\". Example: >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE >3HHU_A Chain A, Human Heat-Shock Protein 90 (Hsp90) MPEETQTQDQPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKL DSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYS AYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKH SQFIGYPITLFVEK The fastq format # The fastq format is also a text based format to represent nucleotide sequences, but also contains the corresponding quality of each nucleotide. It is the standard for storing the output of high-throughput sequencing instruments such as the Illumina machines. A fastq file uses four lines per sequence: Line 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line). Line 2 is the raw sequence letters. Line 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again. Line 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence. An example sequence in fastq format: @SEQ_ID GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + !''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65 Quality # The quality, also called phred score, is the probability that the corresponding basecall is incorrect. Phred scores use a logarithmic scale, and are represented by ASCII characters, mapping to a quality usually going from 0 to 40. Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999% the sam/bam format # From Wikipedia : SAM (file format) is a text-based format for storing biological sequences aligned to a reference sequence developed by Heng Li. The acronym SAM stands for Sequence Alignment/Map. It is widely used for storing data, such as nucleotide sequences, generated by Next generation sequencing technologies and usually mapped to a reference. The SAM format consists of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file. SAM files can be analysed and edited with the software SAMtools. The SAM format has a really extensive and complex specification that you can find here . In brief it consists of a header section and reads (with other information) in tab delimited format. Example header section # @HD VN:1.0 SO:unsorted @SQ SN:O_volvulusOVOC_OM1a LN:2816604 @SQ SN:O_volvulusOVOC_OM1b LN:28345163 @SQ SN:O_volvulusOVOC_OM2 LN:25485961 Example read # M01137:130:00-A:17009:1352/14 * 0 0 * * 0 0 AGCAAAATACAACGATCTGGATGGTAGCATTAGCGATGCGACACTGCTTGAACCGTCAAAG FGGFGCFGFFGC8,,@D?E6EFCF,=AEFFGGDGGGADFGG@>FFEGGG:+<7D>AFCFGG YT:Z:UU the vcf format # The vcf format is also a text-based file format. VCF stands for Variant Call Format and is used to store gene sequence variations (SNVs, indels). The format has been developped for genotyping projects, and is the standard to represent variations in the genome of a species. A vcf is a tab-delimited file, described here . VCF Example # ##fileformat=VCFv4.0 ##fileDate=20110705 ##reference=1000GenomesPilot-NCBI37 ##phasing=partial ##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\"> ##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\"> ##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\"> ##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\"> ##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\"> ##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\"> ##FILTER=<ID=q10,Description=\"Quality below 10\"> ##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\"> ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\"> ##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\"> ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"> ##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\"> #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample1 Sample2 Sample3 2 4370 rs6057 G A 29 . NS=2;DP=13;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:52,51 1|0:48:8:51,51 1/1:43:5:.,. 2 7330 . T A 3 q10 NS=5;DP=12;AF=0.017 GT:GQ:DP:HQ 0|0:46:3:58,50 0|1:3:5:65,3 0/0:41:3 2 110696 rs6055 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:4 2 130237 . T . 47 . NS=2;DP=16;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:56,51 0/0:61:2 2 134567 microsat1 GTCT G,GTACT 50 PASS NS=2;DP=9;AA=G GT:GQ:DP 0/1:35:4 0/2:17:2 1/1:40:3 chr1 45796269 . G C chr1 45797505 . C G chr1 45798555 . T C chr1 45798901 . C T chr1 45805566 . G C chr2 47703379 . C T chr2 48010488 . G A chr2 48030838 . A T chr2 48032875 . CTAT - chr2 48032937 . T C chr2 48033273 . TTTTTGTTTTAATTCCT - chr2 48033551 . C G chr2 48033910 . A T chr2 215632048 . G T chr2 215632125 . TT - chr2 215632155 . T C chr2 215632192 . G A chr2 215632255 . CA TG chr2 215634055 . C T the gff format # The general feature format (gff) is another text file format, used for describing genes and other features of DNA, RNA and protein sequences. It is the standard for annotation of genomes. A gff file should contain 9 columns, described here Example gff # ##description: evidence-based annotation of the human genome (GRCh38), version 25 (Ensembl 85) ##provider: GENCODE ##contact: gencode-help@sanger.ac.uk ##format: gtf ##date: 2016-07-15 chr1 HAVANA gene 11869 14409 . + . gene_id \"ENSG00000223972.5\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\"; chr1 HAVANA transcript 11869 14409 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 11869 12227 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 1; exon_id \"ENSE00002234944.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 12613 12721 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 2; exon_id \"ENSE00003582793.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 13221 14409 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 3; exon_id \"ENSE00002312635.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";","title":"File formats"},{"location":"file_formats/#file-formats","text":"This lecture is aimed at making you discover the most popular file formats used in bioinformatics. You're expected to have basic working knowledge of Linux to be able to follow the lesson.","title":"File Formats"},{"location":"file_formats/#table-of-contents","text":"The fasta format The fastq format The sam/bam format The vcf format The gff format","title":"Table of Contents"},{"location":"file_formats/#the-fasta-format","text":"The fasta format was invented in 1988 and designed to represent nucleotide or peptide sequences. It originates from the FASTA software package, but is now a standard in the world of bioinformatics. The first line in a FASTA file starts with a \">\" (greater-than) symbol followed by the description or identifier of the sequence. Following the initial line (used for a unique description of the sequence) is the actual sequence itself in standard one-letter code. A few sample sequences: >KX580312.1 Homo sapiens truncated breast cancer 1 (BRCA1) gene, exon 15 and partial cds GTCATCCCCTTCTAAATGCCCATCATTAGATGATAGGTGGTACATGCACAGTTGCTCTGGGAGTCTTCAG AATAGAAACTACCCATCTCAAGAGGAGCTCATTAAGGTTGTTGATGTGGAGGAGTAACAGCTGGAAGAGT CTGGGCCACACGATTTGACGGAAACATCTTACTTGCCAAGGCAAGATCTAG >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE A fasta file can contain multiple sequence. Each sequence will be separated by their \"header\" line, starting by \">\". Example: >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE >3HHU_A Chain A, Human Heat-Shock Protein 90 (Hsp90) MPEETQTQDQPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKL DSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYS AYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKH SQFIGYPITLFVEK","title":"The fasta format"},{"location":"file_formats/#the-fastq-format","text":"The fastq format is also a text based format to represent nucleotide sequences, but also contains the corresponding quality of each nucleotide. It is the standard for storing the output of high-throughput sequencing instruments such as the Illumina machines. A fastq file uses four lines per sequence: Line 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line). Line 2 is the raw sequence letters. Line 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again. Line 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence. An example sequence in fastq format: @SEQ_ID GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + !''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65","title":"The fastq format"},{"location":"file_formats/#quality","text":"The quality, also called phred score, is the probability that the corresponding basecall is incorrect. Phred scores use a logarithmic scale, and are represented by ASCII characters, mapping to a quality usually going from 0 to 40. Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999%","title":"Quality"},{"location":"file_formats/#the-sambam-format","text":"From Wikipedia : SAM (file format) is a text-based format for storing biological sequences aligned to a reference sequence developed by Heng Li. The acronym SAM stands for Sequence Alignment/Map. It is widely used for storing data, such as nucleotide sequences, generated by Next generation sequencing technologies and usually mapped to a reference. The SAM format consists of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file. SAM files can be analysed and edited with the software SAMtools. The SAM format has a really extensive and complex specification that you can find here . In brief it consists of a header section and reads (with other information) in tab delimited format.","title":"the sam/bam format"},{"location":"file_formats/#example-header-section","text":"@HD VN:1.0 SO:unsorted @SQ SN:O_volvulusOVOC_OM1a LN:2816604 @SQ SN:O_volvulusOVOC_OM1b LN:28345163 @SQ SN:O_volvulusOVOC_OM2 LN:25485961","title":"Example header section"},{"location":"file_formats/#example-read","text":"M01137:130:00-A:17009:1352/14 * 0 0 * * 0 0 AGCAAAATACAACGATCTGGATGGTAGCATTAGCGATGCGACACTGCTTGAACCGTCAAAG FGGFGCFGFFGC8,,@D?E6EFCF,=AEFFGGDGGGADFGG@>FFEGGG:+<7D>AFCFGG YT:Z:UU","title":"Example read"},{"location":"file_formats/#the-vcf-format","text":"The vcf format is also a text-based file format. VCF stands for Variant Call Format and is used to store gene sequence variations (SNVs, indels). The format has been developped for genotyping projects, and is the standard to represent variations in the genome of a species. A vcf is a tab-delimited file, described here .","title":"the vcf format"},{"location":"file_formats/#vcf-example","text":"##fileformat=VCFv4.0 ##fileDate=20110705 ##reference=1000GenomesPilot-NCBI37 ##phasing=partial ##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\"> ##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\"> ##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\"> ##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\"> ##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\"> ##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\"> ##FILTER=<ID=q10,Description=\"Quality below 10\"> ##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\"> ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\"> ##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\"> ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"> ##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\"> #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample1 Sample2 Sample3 2 4370 rs6057 G A 29 . NS=2;DP=13;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:52,51 1|0:48:8:51,51 1/1:43:5:.,. 2 7330 . T A 3 q10 NS=5;DP=12;AF=0.017 GT:GQ:DP:HQ 0|0:46:3:58,50 0|1:3:5:65,3 0/0:41:3 2 110696 rs6055 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:4 2 130237 . T . 47 . NS=2;DP=16;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:56,51 0/0:61:2 2 134567 microsat1 GTCT G,GTACT 50 PASS NS=2;DP=9;AA=G GT:GQ:DP 0/1:35:4 0/2:17:2 1/1:40:3 chr1 45796269 . G C chr1 45797505 . C G chr1 45798555 . T C chr1 45798901 . C T chr1 45805566 . G C chr2 47703379 . C T chr2 48010488 . G A chr2 48030838 . A T chr2 48032875 . CTAT - chr2 48032937 . T C chr2 48033273 . TTTTTGTTTTAATTCCT - chr2 48033551 . C G chr2 48033910 . A T chr2 215632048 . G T chr2 215632125 . TT - chr2 215632155 . T C chr2 215632192 . G A chr2 215632255 . CA TG chr2 215634055 . C T","title":"VCF Example"},{"location":"file_formats/#the-gff-format","text":"The general feature format (gff) is another text file format, used for describing genes and other features of DNA, RNA and protein sequences. It is the standard for annotation of genomes. A gff file should contain 9 columns, described here","title":"the gff format"},{"location":"file_formats/#example-gff","text":"##description: evidence-based annotation of the human genome (GRCh38), version 25 (Ensembl 85) ##provider: GENCODE ##contact: gencode-help@sanger.ac.uk ##format: gtf ##date: 2016-07-15 chr1 HAVANA gene 11869 14409 . + . gene_id \"ENSG00000223972.5\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\"; chr1 HAVANA transcript 11869 14409 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 11869 12227 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 1; exon_id \"ENSE00002234944.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 12613 12721 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 2; exon_id \"ENSE00003582793.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 13221 14409 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 3; exon_id \"ENSE00002312635.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";","title":"Example gff"},{"location":"mkdocs_usage/","text":"Welcome to MkDocs # For full documentation visit mkdocs.org . Commands # mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout # mkdocs.yml # The configuration file. docs/ usage.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Mkdocs usage"},{"location":"mkdocs_usage/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"mkdocs_usage/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"mkdocs_usage/#project-layout","text":"mkdocs.yml # The configuration file. docs/ usage.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"project_organisation/","text":"Project organization and management # Most of the the project organization material can be found at https://software-carpentry.org and http://www.datacarpentry.org Many thanks to them for existing! Structure or architecture of a data science project # Some good practice when you will organise your project directory on the server, on the cloud or any other machine where you will compute: Create 3 or 4 different directories within you project directory (use mkdir ): data/ for keeping the raw data results/ for all the outputs from the multiple analyses that you will perform docs/ for all the notes written about the analyses carried out (ex: history > 20221114.logs for the commands executed today) scripts/ for all the scripts that you will use to produce the results Note You should always have the raw data in (at least) one place and not modify them More about data structure and metadata # direct link to the tutorial used fo the lesson: Shell genomics: project organisation good practice for the structure of data and metadata of a genomics project: Organisation of genomics project Some extra material: Spreadsheet ecology lesson Exercise # This exercise combines the knowledge you have acquired during the unix and project organisation lessons. You have designed an experiment where you are studying the species and weight of animals caught in plots in a study area. Data was collected by a third party a deposited in figshare , a public database. Our goals are to download and exploring the data, while keeping an organised project directory Set up # First we go to our working directory for this training and create a project directory cd ~/bioinfo_training mkdir animals cd animals As we saw during the project organization tutorial, it is good practice to separate data, results and scripts. Let us create those three directories mkdir data results scripts Downloading the data # First we go to our data directory cd data then we download our data file and give it a more appropriate name wget https://ndownloader.figshare.com/files/2292169 mv 2292169 survey_data.csv Since we'll never modify our raw data file (or at least we do not want to! ) it is safer to remove the writing permissions chmod -w survey_data.csv Note what if my data is really big? Usually when you download data that is several gigabytes large, they will usually be compressed. You learnt about compression during the installing software lesson. Let us look at the first few lines of our file: head data/data_joined.csv Our data file is a .csv file, that is a file where fields are separated by commas , . Each row represent an animal that was caught in a plot, and each column contains information about that animal. Question How many animals do we have? wc -l data/data_joined.csv # 34787 data/data_joined.csv It seems that our dataset contains 34787 lines. Since each line is an animals, we caught a grand total of 34787 animals over the course of our study. Our first analysis script # we saw when we did the head command that all 10 first plots captured rodents. Question Is rodent the only taxon that we have captured? In our csv file, we can see that \"taxa\" is the 12th column. We can print only that column using the cut command cut -d ',' -f 12 data/data_joined.csv | head We still pipe in in head because we do not want to print 34787 line to our screen. Additionally head makes us notice that we still have the column header printed out cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | uniq -c But while uniq is supposed to count all occurrence of a word, it only count similar adjacent occurrences. Before counting, we need to sort our input: cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | sort | uniq -c We see that although we caught a vast majority of rodents, we also caught reptiles, birds and rabbits! Now that we have a working one-liner, let us put it into a script nano scripts/taxa_count.sh and write # script that prints the count of species for csv files cut -d ',' -f 12 \"$1\" | tail -n +2 | sort | uniq -c Saving the result # bash scripts/taxa_count.sh data/data_joined.csv > results/taxa_count.txt cat results/taxa_count.txt Improving our script # We would also like to know the distribution of the numbers of animals caught in plots each year. The year is the 4th column in our dataset and our script, in its current state, always selects the 12th columns of a file. We can change our script to make it flexible so that the user can chose which columns they wishes to work on. nano scripts/taxa_count.sh # script that prints the count of occurrence in one column for csv files cut -d ',' -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c Now it doesn't make much sense to have it named taxa_count.sh mv scripts/taxa_count.sh scripts/column_count.sh Question which year did we catch the most animals? try to answer programmatically. Question save the sorted output to a file in the results directory. Investigating further # We'd like to refine our animal count and knowing how many animals of each taxon were captured every year we can use cut on several columns like this: cut -d ',' -f 4,12 \"data/data_joined.csv\" | tail -n +2 | sort | uniq -c Now that we are happy with our one-liner, let us save it in a script: nano scripts/taxa_per_year.sh then save the output to results/taxa_per_year.txt bash scripts/taxa_per_year.sh > results/taxa_per_year.txt Question Which year was the first reptile captured? The next step would be to refine our analysis by year. We will save one individual output for each year count The seq command # To perform what we want to do, we need to be able to loop over the years. The seq command can help us with that. First we try seq 1 10 then seq 1997 2002 and what about the span of years we are interested in? seq 1977 2002 Great! So now does it work with a for loop? for year in $(seq 1977 2002) do echo $year done It does! Before doing our analysis on each year, we still have to figure out how to do it on one year. grep 1998 results/taxa_per_year.txt \"Grepping\" the year seems to work. Now we need to save it into a file containing the year First let's create a directory where to store our results mkdir results/years and we try to redirect our yearly count into a file grep 1998 results/taxa_per_year.txt > results/years/1998-count.txt bash cat results/years/1998-count.txt It seems to have worked. Now with the loop for year in $(seq 1977 2002) do grep $year results/taxa_per_year.txt > results/years/$year-count.txt done ls results/years Question Put your loop in a script","title":"Project organisation"},{"location":"project_organisation/#project-organization-and-management","text":"Most of the the project organization material can be found at https://software-carpentry.org and http://www.datacarpentry.org Many thanks to them for existing!","title":"Project organization and management"},{"location":"project_organisation/#structure-or-architecture-of-a-data-science-project","text":"Some good practice when you will organise your project directory on the server, on the cloud or any other machine where you will compute: Create 3 or 4 different directories within you project directory (use mkdir ): data/ for keeping the raw data results/ for all the outputs from the multiple analyses that you will perform docs/ for all the notes written about the analyses carried out (ex: history > 20221114.logs for the commands executed today) scripts/ for all the scripts that you will use to produce the results Note You should always have the raw data in (at least) one place and not modify them","title":"Structure or architecture of a data science project"},{"location":"project_organisation/#more-about-data-structure-and-metadata","text":"direct link to the tutorial used fo the lesson: Shell genomics: project organisation good practice for the structure of data and metadata of a genomics project: Organisation of genomics project Some extra material: Spreadsheet ecology lesson","title":"More about data structure and metadata"},{"location":"project_organisation/#exercise","text":"This exercise combines the knowledge you have acquired during the unix and project organisation lessons. You have designed an experiment where you are studying the species and weight of animals caught in plots in a study area. Data was collected by a third party a deposited in figshare , a public database. Our goals are to download and exploring the data, while keeping an organised project directory","title":"Exercise"},{"location":"project_organisation/#set-up","text":"First we go to our working directory for this training and create a project directory cd ~/bioinfo_training mkdir animals cd animals As we saw during the project organization tutorial, it is good practice to separate data, results and scripts. Let us create those three directories mkdir data results scripts","title":"Set up"},{"location":"project_organisation/#downloading-the-data","text":"First we go to our data directory cd data then we download our data file and give it a more appropriate name wget https://ndownloader.figshare.com/files/2292169 mv 2292169 survey_data.csv Since we'll never modify our raw data file (or at least we do not want to! ) it is safer to remove the writing permissions chmod -w survey_data.csv Note what if my data is really big? Usually when you download data that is several gigabytes large, they will usually be compressed. You learnt about compression during the installing software lesson. Let us look at the first few lines of our file: head data/data_joined.csv Our data file is a .csv file, that is a file where fields are separated by commas , . Each row represent an animal that was caught in a plot, and each column contains information about that animal. Question How many animals do we have? wc -l data/data_joined.csv # 34787 data/data_joined.csv It seems that our dataset contains 34787 lines. Since each line is an animals, we caught a grand total of 34787 animals over the course of our study.","title":"Downloading the data"},{"location":"project_organisation/#our-first-analysis-script","text":"we saw when we did the head command that all 10 first plots captured rodents. Question Is rodent the only taxon that we have captured? In our csv file, we can see that \"taxa\" is the 12th column. We can print only that column using the cut command cut -d ',' -f 12 data/data_joined.csv | head We still pipe in in head because we do not want to print 34787 line to our screen. Additionally head makes us notice that we still have the column header printed out cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | uniq -c But while uniq is supposed to count all occurrence of a word, it only count similar adjacent occurrences. Before counting, we need to sort our input: cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | sort | uniq -c We see that although we caught a vast majority of rodents, we also caught reptiles, birds and rabbits! Now that we have a working one-liner, let us put it into a script nano scripts/taxa_count.sh and write # script that prints the count of species for csv files cut -d ',' -f 12 \"$1\" | tail -n +2 | sort | uniq -c","title":"Our first analysis script"},{"location":"project_organisation/#saving-the-result","text":"bash scripts/taxa_count.sh data/data_joined.csv > results/taxa_count.txt cat results/taxa_count.txt","title":"Saving the result"},{"location":"project_organisation/#improving-our-script","text":"We would also like to know the distribution of the numbers of animals caught in plots each year. The year is the 4th column in our dataset and our script, in its current state, always selects the 12th columns of a file. We can change our script to make it flexible so that the user can chose which columns they wishes to work on. nano scripts/taxa_count.sh # script that prints the count of occurrence in one column for csv files cut -d ',' -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c Now it doesn't make much sense to have it named taxa_count.sh mv scripts/taxa_count.sh scripts/column_count.sh Question which year did we catch the most animals? try to answer programmatically. Question save the sorted output to a file in the results directory.","title":"Improving our script"},{"location":"project_organisation/#investigating-further","text":"We'd like to refine our animal count and knowing how many animals of each taxon were captured every year we can use cut on several columns like this: cut -d ',' -f 4,12 \"data/data_joined.csv\" | tail -n +2 | sort | uniq -c Now that we are happy with our one-liner, let us save it in a script: nano scripts/taxa_per_year.sh then save the output to results/taxa_per_year.txt bash scripts/taxa_per_year.sh > results/taxa_per_year.txt Question Which year was the first reptile captured? The next step would be to refine our analysis by year. We will save one individual output for each year count","title":"Investigating further"},{"location":"project_organisation/#the-seq-command","text":"To perform what we want to do, we need to be able to loop over the years. The seq command can help us with that. First we try seq 1 10 then seq 1997 2002 and what about the span of years we are interested in? seq 1977 2002 Great! So now does it work with a for loop? for year in $(seq 1977 2002) do echo $year done It does! Before doing our analysis on each year, we still have to figure out how to do it on one year. grep 1998 results/taxa_per_year.txt \"Grepping\" the year seems to work. Now we need to save it into a file containing the year First let's create a directory where to store our results mkdir results/years and we try to redirect our yearly count into a file grep 1998 results/taxa_per_year.txt > results/years/1998-count.txt bash cat results/years/1998-count.txt It seems to have worked. Now with the loop for year in $(seq 1977 2002) do grep $year results/taxa_per_year.txt > results/years/$year-count.txt done ls results/years Question Put your loop in a script","title":"The seq command"},{"location":"project_organisation_w_git/","text":"Project organization and management # Most of the the project organization material can be found at https://software-carpentry.org and http://www.datacarpentry.org Many thanks to them for existing! Structure or architecture of a data science project # Some good practice when you will organise your project directory on the server, on the cloud or any other machine where you will compute: Create 3 or 4 different directories within you project directory (use mkdir ): data/ for keeping the raw data results/ for all the outputs from the multiple analyses that you will perform docs/ for all the notes written about the analyses carried out (ex: history > 20180125.logs for the commands executed today) scripts/ for all the scripts that you will use to produce the results Note You should always have the raw data in (at least) one place and not modify them More about data structure and metadata # direct link to the tutorial used fo the lesson: Shell genomics: project organisation good practice for the structure of data and metadata of a genomics project: Organisation of genomics project Some extra material: Spreadsheet ecology lesson Exercise # This exercise combines the knowledge you have acquired during the unix , git and project organisation lessons. You have designed an experiment where you are studying the species and weight of animals caught in plots in a study area. Data was collected by a third party a deposited in figshare , a public database. Our goals are to download and exploring the data, while keeping an organised project directory that we will version control using git! Set up # First we go to our Desktop and create a project directory cd ~/Desktop mkdir 2018_animals cd 2018_animals and initialize 2018_animals as a git repository git init As we saw during the project organization tutorial, it is good practice to separate data, results and scripts. Let us create those three directories mkdir data results scripts Downloading the data # First we go to our data directory cd data then we download our data file and give it a more appropriate name wget https://ndownloader.figshare.com/files/2292169 mv 2292169 survey_data.csv Since we'll never modify our raw data file (or at least we do not want to! ) it is safer to remove the writing permissions chmod -w survey_data.csv Additionally since we are now unable to modify it, we do not want to track it in our git repository. We add a .gitignore and tell git to not track the data/ directory nano .gitignore Note what if my data is really big? Usually when you download data that is several gigabytes large, they will usually be compressed. You learnt about compression during the installing software lesson. Let us look at the first few lines of our file: head data/data_joined.csv Our data file is a .csv file, that is a file where fields are separated by commas , . Each row represent an animal that was caught in a plot, and each column contains information about that animal. Question How many animals do we have? wc -l data/data_joined.csv # 34787 data/data_joined.csv It seems that our dataset contains 34787 lines. Since each line is an animals, we caught a grand total of 34787 animals over the course of our study. Our first analysis script # we saw when we did the head command that all 10 first plots captured rodents. Question Is rodent the only taxon that we have captured? In our csv file, we can see that \"taxa\" is the 12th column. We can print only that column using the cut command cut -d ',' -f 12 data/data_joined.csv | head We still pipe in in head because we do not want to print 34787 line to our screen. Additionally head makes us notice that we still have the column header printed out cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | uniq -c But while uniq is supposed to count all occurrence of a word, it only count similar adjacent occurrences. Before counting, we need to sort our input: cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | sort | uniq -c We see that although we caught a vast majority of rodents, we also caught reptiles, birds and rabbits! Now that we have a working one-liner, let us put it into a script nano scripts/taxa_count.sh and write # script that prints the count of species for csv files cut -d ',' -f 12 \"$1\" | tail -n +2 | sort | uniq -c Keeping track of things # Now keep track of your script in git git add scripts/taxa_count.sh git commit -m 'added taxa_count' as well as your gitignore git add .gitignore git commit -m 'added gitignore' Saving the result # bash scripts/taxa_count.sh data/data_joined.csv > results/taxa_count.txt cat results/taxa_count.txt git add results/taxa_count.txt git commit -m 'added results of taxa_count.sh' Improving our script # We would also like to know the distribution of the numbers of animals caught in plots each year. The year is the 4th column in our dataset and our script, in its current state, always selects the 12th columns of a file. We can change our script to make it flexible so that the user can chose which columns they wishes to work on. nano scripts/taxa_count.sh # script that prints the count of occurrence in one column for csv files cut -d ',' -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c Now it doesn't make much sense to have it named taxa_count.sh mv scripts/taxa_count.sh scripts/column_count.sh and let us not forget to keep track of our changes in git! git add -A git commit -m 'made script more flexible about which column to cut on' Question which year did we catch the most animals? try to answer programmatically. Question save the sorted output to a file in the results directory and keep track of it in git. Investigating further # We'd like to refine our animal count and knowing how many animals of each taxon were captured every year we can use cut on several columns like this: cut -d ',' -f 4,12 \"data/data_joined.csv\" | tail -n +2 | sort | uniq -c Now that we are ahhpy with our one-liner, let us save it in a script: nano scripts/taxa_per_year.sh then save the output to results/taxa_per_year.txt bash scripts/taxa_per_year.sh > results/taxa_per_year.txt Question Which year was the first reptile captured? The next step would be to refine our analysis by year. We will save one individual output for each year count The seq command # To perform what we want to do, we need to be able to loop over the years. The seq command can help us with that. First we try seq 1 10 then seq 1997 2002 and what about the span of years we are interested in? seq 1977 2002 Great! So now does it work with a for loop? for year in $(seq 1977 2002) do echo $year done It does! Before doing our analysis on each year, we still have to figure out how to do it on one year. grep 1998 results/taxa_per_year.txt \"Grepping\" the year seems to work. Now we need to save it into a file containing the year First let's create a directory where to store our results mkdir results/years and we try to redirect our yearly count into a file grep 1998 results/taxa_per_year.txt > results/years/1998-count.txt bash cat results/years/1998-count.txt It seems to have worked. Now with the loop for year in $(seq 1977 2002) do grep $year results/taxa_per_year.txt > results/years/$year-count.txt done ls results/years Question Put your loop in a script, and commit everything with git","title":"Project organisation w git"},{"location":"project_organisation_w_git/#project-organization-and-management","text":"Most of the the project organization material can be found at https://software-carpentry.org and http://www.datacarpentry.org Many thanks to them for existing!","title":"Project organization and management"},{"location":"project_organisation_w_git/#structure-or-architecture-of-a-data-science-project","text":"Some good practice when you will organise your project directory on the server, on the cloud or any other machine where you will compute: Create 3 or 4 different directories within you project directory (use mkdir ): data/ for keeping the raw data results/ for all the outputs from the multiple analyses that you will perform docs/ for all the notes written about the analyses carried out (ex: history > 20180125.logs for the commands executed today) scripts/ for all the scripts that you will use to produce the results Note You should always have the raw data in (at least) one place and not modify them","title":"Structure or architecture of a data science project"},{"location":"project_organisation_w_git/#more-about-data-structure-and-metadata","text":"direct link to the tutorial used fo the lesson: Shell genomics: project organisation good practice for the structure of data and metadata of a genomics project: Organisation of genomics project Some extra material: Spreadsheet ecology lesson","title":"More about data structure and metadata"},{"location":"project_organisation_w_git/#exercise","text":"This exercise combines the knowledge you have acquired during the unix , git and project organisation lessons. You have designed an experiment where you are studying the species and weight of animals caught in plots in a study area. Data was collected by a third party a deposited in figshare , a public database. Our goals are to download and exploring the data, while keeping an organised project directory that we will version control using git!","title":"Exercise"},{"location":"project_organisation_w_git/#set-up","text":"First we go to our Desktop and create a project directory cd ~/Desktop mkdir 2018_animals cd 2018_animals and initialize 2018_animals as a git repository git init As we saw during the project organization tutorial, it is good practice to separate data, results and scripts. Let us create those three directories mkdir data results scripts","title":"Set up"},{"location":"project_organisation_w_git/#downloading-the-data","text":"First we go to our data directory cd data then we download our data file and give it a more appropriate name wget https://ndownloader.figshare.com/files/2292169 mv 2292169 survey_data.csv Since we'll never modify our raw data file (or at least we do not want to! ) it is safer to remove the writing permissions chmod -w survey_data.csv Additionally since we are now unable to modify it, we do not want to track it in our git repository. We add a .gitignore and tell git to not track the data/ directory nano .gitignore Note what if my data is really big? Usually when you download data that is several gigabytes large, they will usually be compressed. You learnt about compression during the installing software lesson. Let us look at the first few lines of our file: head data/data_joined.csv Our data file is a .csv file, that is a file where fields are separated by commas , . Each row represent an animal that was caught in a plot, and each column contains information about that animal. Question How many animals do we have? wc -l data/data_joined.csv # 34787 data/data_joined.csv It seems that our dataset contains 34787 lines. Since each line is an animals, we caught a grand total of 34787 animals over the course of our study.","title":"Downloading the data"},{"location":"project_organisation_w_git/#our-first-analysis-script","text":"we saw when we did the head command that all 10 first plots captured rodents. Question Is rodent the only taxon that we have captured? In our csv file, we can see that \"taxa\" is the 12th column. We can print only that column using the cut command cut -d ',' -f 12 data/data_joined.csv | head We still pipe in in head because we do not want to print 34787 line to our screen. Additionally head makes us notice that we still have the column header printed out cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | uniq -c But while uniq is supposed to count all occurrence of a word, it only count similar adjacent occurrences. Before counting, we need to sort our input: cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | sort | uniq -c We see that although we caught a vast majority of rodents, we also caught reptiles, birds and rabbits! Now that we have a working one-liner, let us put it into a script nano scripts/taxa_count.sh and write # script that prints the count of species for csv files cut -d ',' -f 12 \"$1\" | tail -n +2 | sort | uniq -c","title":"Our first analysis script"},{"location":"project_organisation_w_git/#keeping-track-of-things","text":"Now keep track of your script in git git add scripts/taxa_count.sh git commit -m 'added taxa_count' as well as your gitignore git add .gitignore git commit -m 'added gitignore'","title":"Keeping track of things"},{"location":"project_organisation_w_git/#saving-the-result","text":"bash scripts/taxa_count.sh data/data_joined.csv > results/taxa_count.txt cat results/taxa_count.txt git add results/taxa_count.txt git commit -m 'added results of taxa_count.sh'","title":"Saving the result"},{"location":"project_organisation_w_git/#improving-our-script","text":"We would also like to know the distribution of the numbers of animals caught in plots each year. The year is the 4th column in our dataset and our script, in its current state, always selects the 12th columns of a file. We can change our script to make it flexible so that the user can chose which columns they wishes to work on. nano scripts/taxa_count.sh # script that prints the count of occurrence in one column for csv files cut -d ',' -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c Now it doesn't make much sense to have it named taxa_count.sh mv scripts/taxa_count.sh scripts/column_count.sh and let us not forget to keep track of our changes in git! git add -A git commit -m 'made script more flexible about which column to cut on' Question which year did we catch the most animals? try to answer programmatically. Question save the sorted output to a file in the results directory and keep track of it in git.","title":"Improving our script"},{"location":"project_organisation_w_git/#investigating-further","text":"We'd like to refine our animal count and knowing how many animals of each taxon were captured every year we can use cut on several columns like this: cut -d ',' -f 4,12 \"data/data_joined.csv\" | tail -n +2 | sort | uniq -c Now that we are ahhpy with our one-liner, let us save it in a script: nano scripts/taxa_per_year.sh then save the output to results/taxa_per_year.txt bash scripts/taxa_per_year.sh > results/taxa_per_year.txt Question Which year was the first reptile captured? The next step would be to refine our analysis by year. We will save one individual output for each year count","title":"Investigating further"},{"location":"project_organisation_w_git/#the-seq-command","text":"To perform what we want to do, we need to be able to loop over the years. The seq command can help us with that. First we try seq 1 10 then seq 1997 2002 and what about the span of years we are interested in? seq 1977 2002 Great! So now does it work with a for loop? for year in $(seq 1977 2002) do echo $year done It does! Before doing our analysis on each year, we still have to figure out how to do it on one year. grep 1998 results/taxa_per_year.txt \"Grepping\" the year seems to work. Now we need to save it into a file containing the year First let's create a directory where to store our results mkdir results/years and we try to redirect our yearly count into a file grep 1998 results/taxa_per_year.txt > results/years/1998-count.txt bash cat results/years/1998-count.txt It seems to have worked. Now with the loop for year in $(seq 1977 2002) do grep $year results/taxa_per_year.txt > results/years/$year-count.txt done ls results/years Question Put your loop in a script, and commit everything with git","title":"The seq command"},{"location":"qc/","text":"Quality Control and Trimming # Lecture # Practical # In this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data. The first dataset you will be working with is from an Illumina MiSeq dataset. The sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen. The sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011. The sequencing was done as paired-end 2x150bp. Downloading the data # The raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824. You could go to the ENA website and search for the run with the accession SRR957824. However these files contain about 3 million reads and are therefore quite big. We are only going to use a subset of the original dataset for this tutorial. First create a data/ directory in your home folder mkdir ~/data now let's download the subset cd ~/data curl -O -J -L https://osf.io/shqpv/download curl -O -J -L https://osf.io/9m3ch/download Let\u2019s make sure we downloaded all of our data using md5sum. md5sum SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz you should see this 1e8cf249e3217a5a0bcc0d8a654585fb SRR957824_500K_R1.fastq.gz 70c726a31f05f856fe942d727613adb7 SRR957824_500K_R2.fastq.gz and now look at the file names and their size ls -l total 97M -rw-r--r-- 1 hadrien 48M Nov 19 18:44 SRR957824_500K_R1.fastq.gz -rw-r--r-- 1 hadrien 50M Nov 19 18:53 SRR957824_500K_R2.fastq.gz There are 500 000 paired-end reads taken randomly from the original data One last thing before we get to the quality control: those files are writeable. By default, UNIX makes things writeable by the file owner. This poses an issue with creating typos or errors in raw data. We fix that before going further chmod u-w * Working Directory # First we make a work directory: a directory where we can play around with a copy of the data without messing with the original mkdir ~/work cd ~/work Now we make a link of the data in our working directory ln -s ~/data/* . The files that we've downloaded are FASTQ files. Take a look at one of them with zless SRR957824_500K_R1.fastq.gz Tip Use the spacebar to scroll down, and type \u2018q\u2019 to exit \u2018less\u2019 You can read more on the FASTQ format in the File Formats lesson. Question Where does the filename come from? Question Why are there 1 and 2 in the file names? FastQC # To check the quality of the sequence data we will use a tool called FastQC. FastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation. It is available here . However, FastQC is also available as a command line utility on the training server you are using. To run FastQC on our two files fastqc SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz and look what FastQC has produced ls *fastqc* For each file, FastQC has produced both a .zip archive containing all the plots, and a html report. Download and open the html files with your favourite web browser. Alternatively you can look a these copies of them: SRR957824_500K_R1_fastqc.html SRR957824_500K_R2_fastqc.html Question What should you pay attention to in the FastQC report? Question Which file is of better quality? Pay special attention to the per base sequence quality and sequence length distribution. Explanations for the various quality modules can be found here . Also, have a look at examples of a good and a bad illumina read set for comparison. You will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads. Scythe # Now we'll do some trimming! Scythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads. It considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases. The first thing we need is the adapters to trim off curl -O -J -L https://osf.io/v24pt/download Now we run scythe on both our read files scythe -a adapters.fasta SRR957824_500K_R1.fastq.gz -o SRR957824_adapt_R1.fastq scythe -a adapters.fasta SRR957824_500K_R2.fastq.gz -o SRR957824_adapt_R2.fastq Question What adapters do you use? Sickle # Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well. Incorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses. We will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications. To do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold. To run sickle sickle pe -f SRR957824_adapt_R1.fastq -r SRR957824_adapt_R2.fastq \\ -t sanger -o SRR957824_trimmed_R1.fastq -p SRR957824_trimmed_R2.fastq \\ -s /dev/null -q 25 which should output something like PE forward file: SRR957824_trimmed_R1.fastq PE reverse file: SRR957824_trimmed_R2.fastq Total input FastQ records: 1000000 (500000 pairs) FastQ paired records kept: 834570 (417285 pairs) FastQ single records kept: 13263 (from PE1: 11094, from PE2: 2169) FastQ paired records discarded: 138904 (69452 pairs) FastQ single records discarded: 13263 (from PE1: 2169, from PE2: 11094) FastQC again # Run fastqc again on the filtered reads fastqc SRR957824_trimmed_R1.fastq SRR957824_trimmed_R2.fastq and look at the reports SRR957824_trimmed_R1_fastqc.html SRR957824_trimmed_R2_fastqc.html MultiQC # MultiQC is a tool that aggreagtes results from several popular QC bioinformatics software into one html report. Let's run MultiQC in our current directory multiqc . You can download the report or view it by clicking on the link below multiqc_report.html Question What did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?","title":"Short Reads Quality Control and Trimming"},{"location":"qc/#quality-control-and-trimming","text":"","title":"Quality Control and Trimming"},{"location":"qc/#lecture","text":"","title":"Lecture"},{"location":"qc/#practical","text":"In this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data. The first dataset you will be working with is from an Illumina MiSeq dataset. The sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen. The sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011. The sequencing was done as paired-end 2x150bp.","title":"Practical"},{"location":"qc/#downloading-the-data","text":"The raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824. You could go to the ENA website and search for the run with the accession SRR957824. However these files contain about 3 million reads and are therefore quite big. We are only going to use a subset of the original dataset for this tutorial. First create a data/ directory in your home folder mkdir ~/data now let's download the subset cd ~/data curl -O -J -L https://osf.io/shqpv/download curl -O -J -L https://osf.io/9m3ch/download Let\u2019s make sure we downloaded all of our data using md5sum. md5sum SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz you should see this 1e8cf249e3217a5a0bcc0d8a654585fb SRR957824_500K_R1.fastq.gz 70c726a31f05f856fe942d727613adb7 SRR957824_500K_R2.fastq.gz and now look at the file names and their size ls -l total 97M -rw-r--r-- 1 hadrien 48M Nov 19 18:44 SRR957824_500K_R1.fastq.gz -rw-r--r-- 1 hadrien 50M Nov 19 18:53 SRR957824_500K_R2.fastq.gz There are 500 000 paired-end reads taken randomly from the original data One last thing before we get to the quality control: those files are writeable. By default, UNIX makes things writeable by the file owner. This poses an issue with creating typos or errors in raw data. We fix that before going further chmod u-w *","title":"Downloading the data"},{"location":"qc/#working-directory","text":"First we make a work directory: a directory where we can play around with a copy of the data without messing with the original mkdir ~/work cd ~/work Now we make a link of the data in our working directory ln -s ~/data/* . The files that we've downloaded are FASTQ files. Take a look at one of them with zless SRR957824_500K_R1.fastq.gz Tip Use the spacebar to scroll down, and type \u2018q\u2019 to exit \u2018less\u2019 You can read more on the FASTQ format in the File Formats lesson. Question Where does the filename come from? Question Why are there 1 and 2 in the file names?","title":"Working Directory"},{"location":"qc/#fastqc","text":"To check the quality of the sequence data we will use a tool called FastQC. FastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation. It is available here . However, FastQC is also available as a command line utility on the training server you are using. To run FastQC on our two files fastqc SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz and look what FastQC has produced ls *fastqc* For each file, FastQC has produced both a .zip archive containing all the plots, and a html report. Download and open the html files with your favourite web browser. Alternatively you can look a these copies of them: SRR957824_500K_R1_fastqc.html SRR957824_500K_R2_fastqc.html Question What should you pay attention to in the FastQC report? Question Which file is of better quality? Pay special attention to the per base sequence quality and sequence length distribution. Explanations for the various quality modules can be found here . Also, have a look at examples of a good and a bad illumina read set for comparison. You will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads.","title":"FastQC"},{"location":"qc/#scythe","text":"Now we'll do some trimming! Scythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads. It considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases. The first thing we need is the adapters to trim off curl -O -J -L https://osf.io/v24pt/download Now we run scythe on both our read files scythe -a adapters.fasta SRR957824_500K_R1.fastq.gz -o SRR957824_adapt_R1.fastq scythe -a adapters.fasta SRR957824_500K_R2.fastq.gz -o SRR957824_adapt_R2.fastq Question What adapters do you use?","title":"Scythe"},{"location":"qc/#sickle","text":"Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well. Incorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses. We will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications. To do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold. To run sickle sickle pe -f SRR957824_adapt_R1.fastq -r SRR957824_adapt_R2.fastq \\ -t sanger -o SRR957824_trimmed_R1.fastq -p SRR957824_trimmed_R2.fastq \\ -s /dev/null -q 25 which should output something like PE forward file: SRR957824_trimmed_R1.fastq PE reverse file: SRR957824_trimmed_R2.fastq Total input FastQ records: 1000000 (500000 pairs) FastQ paired records kept: 834570 (417285 pairs) FastQ single records kept: 13263 (from PE1: 11094, from PE2: 2169) FastQ paired records discarded: 138904 (69452 pairs) FastQ single records discarded: 13263 (from PE1: 2169, from PE2: 11094)","title":"Sickle"},{"location":"qc/#fastqc-again","text":"Run fastqc again on the filtered reads fastqc SRR957824_trimmed_R1.fastq SRR957824_trimmed_R2.fastq and look at the reports SRR957824_trimmed_R1_fastqc.html SRR957824_trimmed_R2_fastqc.html","title":"FastQC again"},{"location":"qc/#multiqc","text":"MultiQC is a tool that aggreagtes results from several popular QC bioinformatics software into one html report. Let's run MultiQC in our current directory multiqc . You can download the report or view it by clicking on the link below multiqc_report.html Question What did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?","title":"MultiQC"},{"location":"unix/","text":"Introduction to Unix # Most of the introduction to Unix material can be found at https://software-carpentry.org Many thanks to them for existing! Useful resources # Below you will find links to various useful resources for learning or using the UNIX shell. Link to the course material from software carpentry reference of concepts and commands seen during the lesson shell commands explained - a website that shows the help text of any command awesome bash - an awesome list of resources about the bash shell tldp - the linux documentation project (the books can be hard to digest but are very thorough)","title":"The Unix system"},{"location":"unix/#introduction-to-unix","text":"Most of the introduction to Unix material can be found at https://software-carpentry.org Many thanks to them for existing!","title":"Introduction to Unix"},{"location":"unix/#useful-resources","text":"Below you will find links to various useful resources for learning or using the UNIX shell. Link to the course material from software carpentry reference of concepts and commands seen during the lesson shell commands explained - a website that shows the help text of any command awesome bash - an awesome list of resources about the bash shell tldp - the linux documentation project (the books can be hard to digest but are very thorough)","title":"Useful resources"},{"location":"usage/","text":"Welcome to the bioinformatics training for Bacterial Genomics and AMR # You will find on the left menu several tutorials, from basics unix manipulations to more complex HTS data analyses. Thanks and contributors # The tutorials are developed by Juliette Hayer and Lokman Galal (Institut de Recherche pour le D\u00e9veloppement) Some of them are inspired from the teaching materials of Hadrien Gourl\u00e9 ( https://github.com/HadrienG ). Thanks to him :-) We created some of them together during our time at the Swedish University of Agricultural Sciences (SLU). Thanks to SLU, to the SLU Global bioinformatics Centre (SGBC) and to SLUBI ( SLU Bioinformatics Infrastructure )","title":"Main page"},{"location":"usage/#welcome-to-the-bioinformatics-training-for-bacterial-genomics-and-amr","text":"You will find on the left menu several tutorials, from basics unix manipulations to more complex HTS data analyses.","title":"Welcome to the bioinformatics training for Bacterial Genomics and AMR"},{"location":"usage/#thanks-and-contributors","text":"The tutorials are developed by Juliette Hayer and Lokman Galal (Institut de Recherche pour le D\u00e9veloppement) Some of them are inspired from the teaching materials of Hadrien Gourl\u00e9 ( https://github.com/HadrienG ). Thanks to him :-) We created some of them together during our time at the Swedish University of Agricultural Sciences (SLU). Thanks to SLU, to the SLU Global bioinformatics Centre (SGBC) and to SLUBI ( SLU Bioinformatics Infrastructure )","title":"Thanks and contributors"}]}