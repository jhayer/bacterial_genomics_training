{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"annotation/","text":"Genome Annotation # Lecture # After you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation. Prokka is a \"wrapper\"; it collects together several pieces of software (from various authors), and so avoids \"re-inventing the wheel\". Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found here . Prepare our computing environment # We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i You are now on a computing node, with computing 2 cpus reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit Input data # Prokka requires assembled contigs. You can prepare you working directory for this annotation tutorial. cd results ls -l mkdir annotation cd annotation You will link the hybrid assemblies of the 5 Klebsiella strains in this annotation directory: ln -s ../unicycler_assemblies_5_Kp/K*_unicycler_scaffolds.fasta You will also need a proteins set specific of Klebsiella pneumoniae for the annotation. You can go to the Uniprot/Swiss-Prot database website and search for all the proteins sequences for the organism Klebsiella pneumoniae , select only the reviewed entries, and download the fasta file of those. Question How many reviewed protein entries are available for Klebsiella pneumoniae in Swiss-Prot ? For your convenience, we have made the resulting file available. You can copy it by typing the following command: cp /scratch/genesys_training/files/annotation/swissprot_kp_221107.fasta . Running prokka # module load bioinfo/prokka/1.14.6 prokka --force --genus Klebsiella --species pneumoniae \\ --kingdom Bacteria --usegenus --proteins swissprot_kp_221107.fasta \\ --notrna --prefix K2 --outdir K2_prokka K2_unicycler_scaffolds.fasta Once Prokka has finished, examine each of its output files. The GFF and GBK files contain all of the information about the features annotated (in different formats.) The .txt file contains a summary of the number of features annotated. The .faa file contains the protein sequences of the genes annotated. The .ffn file contains the nucleotide sequences of the genes annotated. Preparing Prokka script for loop # We now want to run Prokka on all our strains, so we can compare the annotations later. We will prepare a prokka_kp.sh script for doing so, in the scripts directory. Here is an example: #!/bin/bash ## SLURM CONFIG ## #SBATCH --job-name=prokka #SBATCH --output=%x.%j.out #SBATCH --cpus-per-task 2 #SBATCH --time=24:00:00 #SBATCH -p SELECTED_PARTITION #SBATCH --mail-type=FAIL,END #SBATCH --mem-per-cpu=4G #variables suffix=\"_scaffolds.fasta\" prefix=\"${1%$suffix}\" module load bioinfo/prokka/1.14.6 prokka --force --genus Klebsiella --species pneumoniae \\ --kingdom Bacteria --usegenus --proteins swissprot_kp_221107.fasta \\ --notrna --prefix ${prefix} --outdir ${prefix}_prokka ${1} Once your are ready with your script, you can run the loop for sbatch it: for i in K*scaffolds.fasta; do sbatch ../scripts/prokka_kp.sh $i; done Visualising the annotation # Ugene is a graphical program to perform some bioinformatics analyses. Notably, it allows to browse annotated genomes, and to curate annotations if needed. Download Ugene here and install it on your local computer. Copy the .gff file produced by prokka on your computer, and open it with Ugene.","title":"Annotation"},{"location":"annotation/#genome-annotation","text":"","title":"Genome Annotation"},{"location":"annotation/#lecture","text":"After you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation. Prokka is a \"wrapper\"; it collects together several pieces of software (from various authors), and so avoids \"re-inventing the wheel\". Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found here .","title":"Lecture"},{"location":"annotation/#prepare-our-computing-environment","text":"We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i You are now on a computing node, with computing 2 cpus reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit","title":"Prepare our computing environment"},{"location":"annotation/#input-data","text":"Prokka requires assembled contigs. You can prepare you working directory for this annotation tutorial. cd results ls -l mkdir annotation cd annotation You will link the hybrid assemblies of the 5 Klebsiella strains in this annotation directory: ln -s ../unicycler_assemblies_5_Kp/K*_unicycler_scaffolds.fasta You will also need a proteins set specific of Klebsiella pneumoniae for the annotation. You can go to the Uniprot/Swiss-Prot database website and search for all the proteins sequences for the organism Klebsiella pneumoniae , select only the reviewed entries, and download the fasta file of those. Question How many reviewed protein entries are available for Klebsiella pneumoniae in Swiss-Prot ? For your convenience, we have made the resulting file available. You can copy it by typing the following command: cp /scratch/genesys_training/files/annotation/swissprot_kp_221107.fasta .","title":"Input data"},{"location":"annotation/#running-prokka","text":"module load bioinfo/prokka/1.14.6 prokka --force --genus Klebsiella --species pneumoniae \\ --kingdom Bacteria --usegenus --proteins swissprot_kp_221107.fasta \\ --notrna --prefix K2 --outdir K2_prokka K2_unicycler_scaffolds.fasta Once Prokka has finished, examine each of its output files. The GFF and GBK files contain all of the information about the features annotated (in different formats.) The .txt file contains a summary of the number of features annotated. The .faa file contains the protein sequences of the genes annotated. The .ffn file contains the nucleotide sequences of the genes annotated.","title":"Running prokka"},{"location":"annotation/#preparing-prokka-script-for-loop","text":"We now want to run Prokka on all our strains, so we can compare the annotations later. We will prepare a prokka_kp.sh script for doing so, in the scripts directory. Here is an example: #!/bin/bash ## SLURM CONFIG ## #SBATCH --job-name=prokka #SBATCH --output=%x.%j.out #SBATCH --cpus-per-task 2 #SBATCH --time=24:00:00 #SBATCH -p SELECTED_PARTITION #SBATCH --mail-type=FAIL,END #SBATCH --mem-per-cpu=4G #variables suffix=\"_scaffolds.fasta\" prefix=\"${1%$suffix}\" module load bioinfo/prokka/1.14.6 prokka --force --genus Klebsiella --species pneumoniae \\ --kingdom Bacteria --usegenus --proteins swissprot_kp_221107.fasta \\ --notrna --prefix ${prefix} --outdir ${prefix}_prokka ${1} Once your are ready with your script, you can run the loop for sbatch it: for i in K*scaffolds.fasta; do sbatch ../scripts/prokka_kp.sh $i; done","title":"Preparing Prokka script for loop"},{"location":"annotation/#visualising-the-annotation","text":"Ugene is a graphical program to perform some bioinformatics analyses. Notably, it allows to browse annotated genomes, and to curate annotations if needed. Download Ugene here and install it on your local computer. Copy the .gff file produced by prokka on your computer, and open it with Ugene.","title":"Visualising the annotation"},{"location":"args/","text":"Antimicrobial Resistance Genes and Mutations detection # We will be using 2 different tools and their associated databases for detecting genes and mutations that could confer an antimicrobial resistance to the bacteria carrying it. These tools are: AMRFinderPlus from NCBI CARD RGI (Resistance Gene Identifier) In this practical you will learn to run these tools and their associated tools, to inspect the output files, and to visualise them in a graphical way. Prepare our computing environment # We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i You are now on a computing node, with computing 2 cpus reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit First, you need to identify in your results directory the fasta file corresponding to the unicycler hybrid assembly. cd results ls -l AMRFinderPlus # First we will activate the AMRFinder container, and check the options and the different organisms datasets available for resistance mutations detection module load system/singularity/3.6.0 singularity shell path/to/amrfinder_container amrfinder -h amrfinder --list_organisms Now that we have identified the organism option to use, we can run the command amrfinder --nucleotide K2_unicycler_scaffolds.fasta -o K2_unicycler_AMRfinder.txt --organism Klebsiella --plus --mutation_all K2_unicycler_AMRfinder_all_mut.txt Question Inspect both output files and comment. Which resistance could be identified ? Now that we are more used to the command line, we can prepare a generic script amrfinder_script.sh to run AMRFinderPlus on all our strains draft genomes, using a loop for sbatch the jobs. Create it in the scripts directory. Here is an example of script that we will explain, and where you need to change some paths... #!/bin/bash ## SLURM CONFIG ## #SBATCH --job-name=amrfinder #SBATCH --output=%x.%j.out #SBATCH --cpus-per-task 4 #SBATCH --time=24:00:00 #SBATCH -p SELECTED_PARTITION #SBATCH --mail-type=FAIL,END #SBATCH --mem-per-cpu=4G ### HELP=\"USAGE: amrfinderplus.sh contigs.fasta\" # If we didn't get any arguments, print help and exit if [[ $# < 1 ]] then echo \"$HELP\" exit 0 fi #variables suffix=\".fasta\" prefix1=\"${1%$suffix}\" ## module load system/singularity/3.6.0 singularity run path/to/amrfinder_container \\ amrfinder --nucleotide ${1} -o ${prefix1}_AMRfinder.txt --organism Klebsiella --plus --mutation_all ${prefix1}_AMRfinder_all_mut.txt Once your script seems ready, exit the container, and the srun if you are still on the node. Then you are ready to loop on the unicycler fasta files for running the script with sbatch First, we can test it on one strain: sbatch ../scripts/amrfinder_script.sh K1_unicycler_scaffolds.fasta If the test has worked, you can run the loop: for i in K*_unicycler_scaffolds.fasta; \\ do sbatch ../scripts/amrfinder_script.sh $i;\\ done CARD RGI # Now that we feel more familiar with preparing SLURM scripts, we will make card_script.sh for running CARD RGI on all samples. #!/bin/bash ## SLURM CONFIG ## #SBATCH --job-name=card #SBATCH --output=%x.%j.out #SBATCH --cpus-per-task 4 #SBATCH --time=24:00:00 #SBATCH -p highmemplus #SBATCH --mail-type=FAIL,END #SBATCH --mem-per-cpu=4G ### HELP=\" USAGE: card_script.sh contigs.fasta \" # If we didn't get any arguments, print help and exit if [[ $# < 1 ]] then echo \"$HELP\" exit 0 fi #variables suffix=\".fasta\" prefix1=\"${1%$suffix}\" ## module load system/singularity/3.6.0 # loading the CARD database singularity run /path/to/containers/rgi_6.0.0--pyha8f3691_0.sif \\ rgi load --card_json /path/to/databases/CARD/card.json # Running RGI Main => output json for Heatmap plotting singularity run /path/to/containers/rgi_6.0.0--pyha8f3691_0.sif \\ rgi main -i ${1} -o ${prefix1}_RGI_main --debug -a BLAST -d wgs -n 4 # Preparing tabular file for import to excel or R singularity run /path/to/containers/rgi_6.0.0--pyha8f3691_0.sif \\ rgi tab -i ${prefix1}_RGI_main.json rm ${1}.temp.* rm ${1}.temp Make a test run on one sample before running the loop. If the test has worked, you can run the loop: for i in K*_unicycler_scaffolds.fasta; \\ do sbatch ../scripts/card_script.sh $i;\\ done Once we have all the json files from CARD RGI, we will place them in the same directory. mkdir card_json mv *_RGI_main.json card_json After that, we can run the RGI program for making the heatmap clustering Run the srun command if you are not already on the computing node: srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i Run RGI heatmap: module load system/singularity/3.6.0 singularity run path/to/card_container #Generate a heat map from pre-compiled RGI main JSON files, samples clustered by similarity of resistome and AMR genes organized by Drug Class rgi heatmap -i card_json -cat drug_class -o hm_drug_class -clus samples Tip You can find other ways to cluster/classify the resistance genes, as the following examples: #Generate a heat map from pre-compiled RGI main JSON files, samples clustered by similarity of resistome and AMR genes organized by resistance resistance_mechanism rgi heatmap -i card_json -cat resistance_mechanism -o hm_resistance_mechanism -clus samples #Generate a heat map from pre-compiled RGI main JSON files, samples clustered by similarity of resistome and AMR genes organized by AMR gene family rgi heatmap -i card_json -cat gene_family -o hm_genefamily_samples -clus samples #Generate a heat map from pre-compiled RGI main JSON files, samples clustered by similarity of resistome (with histogram used for abundance of identical resistomes) and AMR genes organized by distribution among samples: rgi heatmap -i card_json -o cluster_both_frequency -f -clus both Then you can scp all the pictures on you computer and inspect.","title":"Antimicrobial Resistance Genes"},{"location":"args/#antimicrobial-resistance-genes-and-mutations-detection","text":"We will be using 2 different tools and their associated databases for detecting genes and mutations that could confer an antimicrobial resistance to the bacteria carrying it. These tools are: AMRFinderPlus from NCBI CARD RGI (Resistance Gene Identifier) In this practical you will learn to run these tools and their associated tools, to inspect the output files, and to visualise them in a graphical way.","title":"Antimicrobial Resistance Genes and Mutations detection"},{"location":"args/#prepare-our-computing-environment","text":"We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i You are now on a computing node, with computing 2 cpus reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit First, you need to identify in your results directory the fasta file corresponding to the unicycler hybrid assembly. cd results ls -l","title":"Prepare our computing environment"},{"location":"args/#amrfinderplus","text":"First we will activate the AMRFinder container, and check the options and the different organisms datasets available for resistance mutations detection module load system/singularity/3.6.0 singularity shell path/to/amrfinder_container amrfinder -h amrfinder --list_organisms Now that we have identified the organism option to use, we can run the command amrfinder --nucleotide K2_unicycler_scaffolds.fasta -o K2_unicycler_AMRfinder.txt --organism Klebsiella --plus --mutation_all K2_unicycler_AMRfinder_all_mut.txt Question Inspect both output files and comment. Which resistance could be identified ? Now that we are more used to the command line, we can prepare a generic script amrfinder_script.sh to run AMRFinderPlus on all our strains draft genomes, using a loop for sbatch the jobs. Create it in the scripts directory. Here is an example of script that we will explain, and where you need to change some paths... #!/bin/bash ## SLURM CONFIG ## #SBATCH --job-name=amrfinder #SBATCH --output=%x.%j.out #SBATCH --cpus-per-task 4 #SBATCH --time=24:00:00 #SBATCH -p SELECTED_PARTITION #SBATCH --mail-type=FAIL,END #SBATCH --mem-per-cpu=4G ### HELP=\"USAGE: amrfinderplus.sh contigs.fasta\" # If we didn't get any arguments, print help and exit if [[ $# < 1 ]] then echo \"$HELP\" exit 0 fi #variables suffix=\".fasta\" prefix1=\"${1%$suffix}\" ## module load system/singularity/3.6.0 singularity run path/to/amrfinder_container \\ amrfinder --nucleotide ${1} -o ${prefix1}_AMRfinder.txt --organism Klebsiella --plus --mutation_all ${prefix1}_AMRfinder_all_mut.txt Once your script seems ready, exit the container, and the srun if you are still on the node. Then you are ready to loop on the unicycler fasta files for running the script with sbatch First, we can test it on one strain: sbatch ../scripts/amrfinder_script.sh K1_unicycler_scaffolds.fasta If the test has worked, you can run the loop: for i in K*_unicycler_scaffolds.fasta; \\ do sbatch ../scripts/amrfinder_script.sh $i;\\ done","title":"AMRFinderPlus"},{"location":"args/#card-rgi","text":"Now that we feel more familiar with preparing SLURM scripts, we will make card_script.sh for running CARD RGI on all samples. #!/bin/bash ## SLURM CONFIG ## #SBATCH --job-name=card #SBATCH --output=%x.%j.out #SBATCH --cpus-per-task 4 #SBATCH --time=24:00:00 #SBATCH -p highmemplus #SBATCH --mail-type=FAIL,END #SBATCH --mem-per-cpu=4G ### HELP=\" USAGE: card_script.sh contigs.fasta \" # If we didn't get any arguments, print help and exit if [[ $# < 1 ]] then echo \"$HELP\" exit 0 fi #variables suffix=\".fasta\" prefix1=\"${1%$suffix}\" ## module load system/singularity/3.6.0 # loading the CARD database singularity run /path/to/containers/rgi_6.0.0--pyha8f3691_0.sif \\ rgi load --card_json /path/to/databases/CARD/card.json # Running RGI Main => output json for Heatmap plotting singularity run /path/to/containers/rgi_6.0.0--pyha8f3691_0.sif \\ rgi main -i ${1} -o ${prefix1}_RGI_main --debug -a BLAST -d wgs -n 4 # Preparing tabular file for import to excel or R singularity run /path/to/containers/rgi_6.0.0--pyha8f3691_0.sif \\ rgi tab -i ${prefix1}_RGI_main.json rm ${1}.temp.* rm ${1}.temp Make a test run on one sample before running the loop. If the test has worked, you can run the loop: for i in K*_unicycler_scaffolds.fasta; \\ do sbatch ../scripts/card_script.sh $i;\\ done Once we have all the json files from CARD RGI, we will place them in the same directory. mkdir card_json mv *_RGI_main.json card_json After that, we can run the RGI program for making the heatmap clustering Run the srun command if you are not already on the computing node: srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i Run RGI heatmap: module load system/singularity/3.6.0 singularity run path/to/card_container #Generate a heat map from pre-compiled RGI main JSON files, samples clustered by similarity of resistome and AMR genes organized by Drug Class rgi heatmap -i card_json -cat drug_class -o hm_drug_class -clus samples Tip You can find other ways to cluster/classify the resistance genes, as the following examples: #Generate a heat map from pre-compiled RGI main JSON files, samples clustered by similarity of resistome and AMR genes organized by resistance resistance_mechanism rgi heatmap -i card_json -cat resistance_mechanism -o hm_resistance_mechanism -clus samples #Generate a heat map from pre-compiled RGI main JSON files, samples clustered by similarity of resistome and AMR genes organized by AMR gene family rgi heatmap -i card_json -cat gene_family -o hm_genefamily_samples -clus samples #Generate a heat map from pre-compiled RGI main JSON files, samples clustered by similarity of resistome (with histogram used for abundance of identical resistomes) and AMR genes organized by distribution among samples: rgi heatmap -i card_json -o cluster_both_frequency -f -clus both Then you can scp all the pictures on you computer and inspect.","title":"CARD RGI"},{"location":"assembly_hybrid/","text":"Hybrid Genome Assembly using long and short reads data # In this practical we will perform the assembly of Klebsiella pneumoniae , using the short and the long reads that we have trimmed in previous tutorials. Getting the data # We have trimmed the raw short-reads and the long reads in the previous steps of the training, so we will now assemble them into longer sequences called contigs, using a method of hybrid assembly combining both long and short reads, with the Unicycler pipeline. Find your 3 fastq(.gz) files containing the trimmed reads for strain K2. cd results ls -l Unicycler hybrid assembly # As you might have noticed, assemblies can take some time. For that reason, we will prepare a unicycler_assembly_K2.sh script that we will run on the Slurm job scheduler, using the sbatch command. Tip You need to ask the teacher which partition to use ! #!/bin/bash ## SLURM CONFIG ## #SBATCH --job-name=unicycler #SBATCH --output=%x.%j.out #SBATCH --cpus-per-task 4 #SBATCH --time=24:00:00 #SBATCH -p SELECTED_PARTITION #SBATCH --mail-type=FAIL,END #SBATCH --mem-per-cpu=4G ## module load bioinfo/racon/1.4.3 module load bioinfo/unicycler/0.4.4 unicycler -1 K2_Illu_R1_trimmed.fastq -2 K2_Illu_R2_trimmed.fastq --long K2_MinION.fastq.gz \\ -o K2_unicycler_assembly -t 4 When you think that your script is ready, you can run the job with SLURM using this command (exit the srun if it is still active): sbatch unicycler_assembly_K2.sh Then check that your script is \"Running\" by typing: squeue ## to see only your jobs, select the user squeue -u your_login The result of the assembly is in the directory K2_unicycler_assembly under the name assembly.fasta First, have a look of the Unicycler output directory. Question what are the different files there? Check the assembly graph (gfa file) with Bandage => you will need to use the scp command from your computer. Let's now make a link of the file containing the assembled scaffolds, to simplify the run of Quast and BUSCO ln -s K2_unicycler_assembly/assembly.fasta K2_unicycler_scaffolds.fasta and look at it head K2_unicycler_scaffolds.fasta Quality of the Assembly # QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including First we might need to type the srun command to book the resources (the previous step was with sbatch ): srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i Run Quast on your assembly (type the srun command first if this is not done already) module load bioinfo/quast/5.0.2 module load bioinfo/bedtools/2.30.0 module load bioinfo/minimap2/2.24 quast.py -o K2_unicycler_quast -t 2 --conserved-genes-finding --gene-finding \\ --pe1 K2_Illu_trimmed_R1.fastq.gz --pe2 K2_Illu_trimmed_R2.fastq.gz K2_unicycler_scaffolds.fasta and take a look at the text report cat K2_unicycler_quast/report.txt Question How well does the assembly total size and coverage correspond to your earlier estimation? Question How many contigs in total did the assembly produce? Question Has the assembly improved compared with short-reads only assembly? in term of N50 and L50 for example. Let's now check the completeness in term of essential genes expected with BUSCO Assembly Completeness # You can find more info regarding BUSCO in the short-reads assembly tutorial. Let's run it ! module load bioinfo/BUSCO/5.2.2 busco -i K2_unicycler_scaffolds.fasta -o K2_unicycler_busco --mode genome --lineage_dataset enterobacterales_odb10 Question How many marker genes has busco found? Was this number improved compared to previous assemblies of short-reads only and long-reads only ?","title":"Hybrid assembly"},{"location":"assembly_hybrid/#hybrid-genome-assembly-using-long-and-short-reads-data","text":"In this practical we will perform the assembly of Klebsiella pneumoniae , using the short and the long reads that we have trimmed in previous tutorials.","title":"Hybrid Genome Assembly using long and short reads data"},{"location":"assembly_hybrid/#getting-the-data","text":"We have trimmed the raw short-reads and the long reads in the previous steps of the training, so we will now assemble them into longer sequences called contigs, using a method of hybrid assembly combining both long and short reads, with the Unicycler pipeline. Find your 3 fastq(.gz) files containing the trimmed reads for strain K2. cd results ls -l","title":"Getting the data"},{"location":"assembly_hybrid/#unicycler-hybrid-assembly","text":"As you might have noticed, assemblies can take some time. For that reason, we will prepare a unicycler_assembly_K2.sh script that we will run on the Slurm job scheduler, using the sbatch command. Tip You need to ask the teacher which partition to use ! #!/bin/bash ## SLURM CONFIG ## #SBATCH --job-name=unicycler #SBATCH --output=%x.%j.out #SBATCH --cpus-per-task 4 #SBATCH --time=24:00:00 #SBATCH -p SELECTED_PARTITION #SBATCH --mail-type=FAIL,END #SBATCH --mem-per-cpu=4G ## module load bioinfo/racon/1.4.3 module load bioinfo/unicycler/0.4.4 unicycler -1 K2_Illu_R1_trimmed.fastq -2 K2_Illu_R2_trimmed.fastq --long K2_MinION.fastq.gz \\ -o K2_unicycler_assembly -t 4 When you think that your script is ready, you can run the job with SLURM using this command (exit the srun if it is still active): sbatch unicycler_assembly_K2.sh Then check that your script is \"Running\" by typing: squeue ## to see only your jobs, select the user squeue -u your_login The result of the assembly is in the directory K2_unicycler_assembly under the name assembly.fasta First, have a look of the Unicycler output directory. Question what are the different files there? Check the assembly graph (gfa file) with Bandage => you will need to use the scp command from your computer. Let's now make a link of the file containing the assembled scaffolds, to simplify the run of Quast and BUSCO ln -s K2_unicycler_assembly/assembly.fasta K2_unicycler_scaffolds.fasta and look at it head K2_unicycler_scaffolds.fasta","title":"Unicycler hybrid assembly"},{"location":"assembly_hybrid/#quality-of-the-assembly","text":"QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including First we might need to type the srun command to book the resources (the previous step was with sbatch ): srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i Run Quast on your assembly (type the srun command first if this is not done already) module load bioinfo/quast/5.0.2 module load bioinfo/bedtools/2.30.0 module load bioinfo/minimap2/2.24 quast.py -o K2_unicycler_quast -t 2 --conserved-genes-finding --gene-finding \\ --pe1 K2_Illu_trimmed_R1.fastq.gz --pe2 K2_Illu_trimmed_R2.fastq.gz K2_unicycler_scaffolds.fasta and take a look at the text report cat K2_unicycler_quast/report.txt Question How well does the assembly total size and coverage correspond to your earlier estimation? Question How many contigs in total did the assembly produce? Question Has the assembly improved compared with short-reads only assembly? in term of N50 and L50 for example. Let's now check the completeness in term of essential genes expected with BUSCO","title":"Quality of the Assembly"},{"location":"assembly_hybrid/#assembly-completeness","text":"You can find more info regarding BUSCO in the short-reads assembly tutorial. Let's run it ! module load bioinfo/BUSCO/5.2.2 busco -i K2_unicycler_scaffolds.fasta -o K2_unicycler_busco --mode genome --lineage_dataset enterobacterales_odb10 Question How many marker genes has busco found? Was this number improved compared to previous assemblies of short-reads only and long-reads only ?","title":"Assembly Completeness"},{"location":"assembly_sr/","text":"De-novo Genome Assembly # Lecture # Practical # In this practical we will perform the assembly of Klebsiella pneumoniae , using the reads that we have trimmed in the previous Quality Control tutorial. Prepare our computing environment # We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i You are now on a computing node, with computing 2 cpus reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit Getting the data # We have trimmed the raw short-reads in the previous step of the training, so we will now assemble them into longer sequences called contigs. Find your 2 fastq files containing the trimmed reads. cd results ls -l Question How many reads are in the files? De-novo assembly # We will be using the SPAdes assembler to assemble our bacterium module load bioinfo/SPAdes/3.15.3 spades.py -1 K2_Illu_R1_trimmed.fastq -2 K2_Illu_R2_trimmed.fastq -o K2_spades_assembly -t 4 --isolate This will take some time... Because we know that this will take quite some time, we better put the command in a SLURM script that we can sbatch . In that way, we can log out and come back later, the job will keep running. You can prepare the script spades_assembly_K2.sh , this is an example to help you. Tip You need to ask the teacher which partition to use ! #!/bin/bash ## SLURM CONFIG ## #SBATCH --job-name=spades #SBATCH --output=%x.%j.out #SBATCH --cpus-per-task 4 #SBATCH --time=24:00:00 #SBATCH -p SELECTED_PARTITION #SBATCH --mail-type=FAIL,END #SBATCH --mem-per-cpu=4G ## module load bioinfo/SPAdes/3.15.3 spades.py -1 K2_Illu_R1_trimmed.fastq -2 K2_Illu_R2_trimmed.fastq -o K2_spades_assembly -t 4 --isolate When you think that your script is ready, you can run the job with SLURM using this command (exit the srun if it is still active): sbatch spades_assembly_K2.sh Then check that your script is \"Running\" by typing: squeue ## to see only your jobs, select the user squeue -u your_login Be careful and put the correct name in \"your_login\". The result of the assembly is in the directory K2_spades_assembly under the name scaffolds.fasta First, have a look of the SPAdes output directory. Question what are the different files there? Check the assembly graph (gfa file) with Bandage => you will need to use the scp command from your computer. Tip We have made it available, you can get the graph file by typing (when you are on the appropriate node): cp /scratch/genesys_training/files/... . Let's make a link of the file containing the assembled scaffolds ln -s K2_spades_assembly/scaffolds.fasta K2_spades_scaffolds.fasta and look at it head K2_spades_scaffolds.fasta Quality of the Assembly # QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including Run Quast on your assembly (type the srun command first if this is not done already) module load bioinfo/quast/5.0.2 module load bioinfo/bedtools/2.30.0 module load bioinfo/minimap2/2.24 quast.py -o K2_spades_quast -t 2 --conserved-genes-finding --gene-finding \\ --pe1 K2_Illu_R1_trimmed.fastq --pe2 K2_Illu_R2_trimmed.fastq K2_spades_scaffolds.fasta and take a look at the text report cat K2_spades_quast/report.txt You should see something like Assembly K2_spades_scaffolds # contigs (>= 0 bp) 3392 # contigs (>= 1000 bp) 447 # contigs (>= 5000 bp) 46 # contigs (>= 10000 bp) 40 # contigs (>= 25000 bp) 24 # contigs (>= 50000 bp) 19 Total length (>= 0 bp) 7774823 Total length (>= 1000 bp) 6044278 Total length (>= 5000 bp) 5470941 Total length (>= 10000 bp) 5428782 Total length (>= 25000 bp) 5151491 Total length (>= 50000 bp) 4973703 # contigs 2408 Largest contig 815843 Total length 7335510 GC (%) 57.04 N50 252444 N75 3008 L50 9 L75 55 # total reads 1129033 # left 559060 # right 559060 Mapped (%) 99.46 Properly paired (%) 98.12 Avg. coverage depth 35 Coverage >= 1x (%) 100.0 # N's per 100 kbp 0.00 which is a summary stats about our assembly. Additionally, the file K2_spades_quast/report.html You can either download it and open it in your own web browser, or we make it available for your convenience: K2_spades_quast_report.html Question How well does the assembly total consensus size and coverage correspond to your earlier estimation? Question How many contigs in total did the assembly produce? Question What is the N50 of the assembly? What does this mean? Assembly Completeness # Although quast output a range of metric to assess how contiguous our assembly is, having a long N50 does not guarantee a good assembly: it could be riddled by misassemblies! We will run busco to try to find marker genes in our assembly. Marker genes are conserved across a range of species and finding intact conserved genes in our assembly would be a good indication of its quality First we need to check if the bacterial datasets are available for busco and which ones are available module load bioinfo/BUSCO/5.2.2 busco --list-datasets Question Which dataset should we select? then we can run busco with: busco -i K2_spades_scaffolds.fasta -o K2_spades_busco --mode genome --lineage_dataset enterobacterales_odb10 Question How many marker genes has busco found? Course literature # Course literature for today is: Next-Generation Sequence Assembly: Four Stages of Data Processing and Computational Challenges: https://doi.org/10.1371/journal.pcbi.1003345","title":"Short reads assembly"},{"location":"assembly_sr/#de-novo-genome-assembly","text":"","title":"De-novo Genome Assembly"},{"location":"assembly_sr/#lecture","text":"","title":"Lecture"},{"location":"assembly_sr/#practical","text":"In this practical we will perform the assembly of Klebsiella pneumoniae , using the reads that we have trimmed in the previous Quality Control tutorial.","title":"Practical"},{"location":"assembly_sr/#prepare-our-computing-environment","text":"We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i You are now on a computing node, with computing 2 cpus reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit","title":"Prepare our computing environment"},{"location":"assembly_sr/#getting-the-data","text":"We have trimmed the raw short-reads in the previous step of the training, so we will now assemble them into longer sequences called contigs. Find your 2 fastq files containing the trimmed reads. cd results ls -l Question How many reads are in the files?","title":"Getting the data"},{"location":"assembly_sr/#de-novo-assembly","text":"We will be using the SPAdes assembler to assemble our bacterium module load bioinfo/SPAdes/3.15.3 spades.py -1 K2_Illu_R1_trimmed.fastq -2 K2_Illu_R2_trimmed.fastq -o K2_spades_assembly -t 4 --isolate This will take some time... Because we know that this will take quite some time, we better put the command in a SLURM script that we can sbatch . In that way, we can log out and come back later, the job will keep running. You can prepare the script spades_assembly_K2.sh , this is an example to help you. Tip You need to ask the teacher which partition to use ! #!/bin/bash ## SLURM CONFIG ## #SBATCH --job-name=spades #SBATCH --output=%x.%j.out #SBATCH --cpus-per-task 4 #SBATCH --time=24:00:00 #SBATCH -p SELECTED_PARTITION #SBATCH --mail-type=FAIL,END #SBATCH --mem-per-cpu=4G ## module load bioinfo/SPAdes/3.15.3 spades.py -1 K2_Illu_R1_trimmed.fastq -2 K2_Illu_R2_trimmed.fastq -o K2_spades_assembly -t 4 --isolate When you think that your script is ready, you can run the job with SLURM using this command (exit the srun if it is still active): sbatch spades_assembly_K2.sh Then check that your script is \"Running\" by typing: squeue ## to see only your jobs, select the user squeue -u your_login Be careful and put the correct name in \"your_login\". The result of the assembly is in the directory K2_spades_assembly under the name scaffolds.fasta First, have a look of the SPAdes output directory. Question what are the different files there? Check the assembly graph (gfa file) with Bandage => you will need to use the scp command from your computer. Tip We have made it available, you can get the graph file by typing (when you are on the appropriate node): cp /scratch/genesys_training/files/... . Let's make a link of the file containing the assembled scaffolds ln -s K2_spades_assembly/scaffolds.fasta K2_spades_scaffolds.fasta and look at it head K2_spades_scaffolds.fasta","title":"De-novo assembly"},{"location":"assembly_sr/#quality-of-the-assembly","text":"QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including Run Quast on your assembly (type the srun command first if this is not done already) module load bioinfo/quast/5.0.2 module load bioinfo/bedtools/2.30.0 module load bioinfo/minimap2/2.24 quast.py -o K2_spades_quast -t 2 --conserved-genes-finding --gene-finding \\ --pe1 K2_Illu_R1_trimmed.fastq --pe2 K2_Illu_R2_trimmed.fastq K2_spades_scaffolds.fasta and take a look at the text report cat K2_spades_quast/report.txt You should see something like Assembly K2_spades_scaffolds # contigs (>= 0 bp) 3392 # contigs (>= 1000 bp) 447 # contigs (>= 5000 bp) 46 # contigs (>= 10000 bp) 40 # contigs (>= 25000 bp) 24 # contigs (>= 50000 bp) 19 Total length (>= 0 bp) 7774823 Total length (>= 1000 bp) 6044278 Total length (>= 5000 bp) 5470941 Total length (>= 10000 bp) 5428782 Total length (>= 25000 bp) 5151491 Total length (>= 50000 bp) 4973703 # contigs 2408 Largest contig 815843 Total length 7335510 GC (%) 57.04 N50 252444 N75 3008 L50 9 L75 55 # total reads 1129033 # left 559060 # right 559060 Mapped (%) 99.46 Properly paired (%) 98.12 Avg. coverage depth 35 Coverage >= 1x (%) 100.0 # N's per 100 kbp 0.00 which is a summary stats about our assembly. Additionally, the file K2_spades_quast/report.html You can either download it and open it in your own web browser, or we make it available for your convenience: K2_spades_quast_report.html Question How well does the assembly total consensus size and coverage correspond to your earlier estimation? Question How many contigs in total did the assembly produce? Question What is the N50 of the assembly? What does this mean?","title":"Quality of the Assembly"},{"location":"assembly_sr/#assembly-completeness","text":"Although quast output a range of metric to assess how contiguous our assembly is, having a long N50 does not guarantee a good assembly: it could be riddled by misassemblies! We will run busco to try to find marker genes in our assembly. Marker genes are conserved across a range of species and finding intact conserved genes in our assembly would be a good indication of its quality First we need to check if the bacterial datasets are available for busco and which ones are available module load bioinfo/BUSCO/5.2.2 busco --list-datasets Question Which dataset should we select? then we can run busco with: busco -i K2_spades_scaffolds.fasta -o K2_spades_busco --mode genome --lineage_dataset enterobacterales_odb10 Question How many marker genes has busco found?","title":"Assembly Completeness"},{"location":"assembly_sr/#course-literature","text":"Course literature for today is: Next-Generation Sequence Assembly: Four Stages of Data Processing and Computational Challenges: https://doi.org/10.1371/journal.pcbi.1003345","title":"Course literature"},{"location":"file_formats/","text":"Lecture sequencing technologies # File Formats # This lecture is aimed at making you discover the most popular file formats used in bioinformatics. You're expected to have basic working knowledge of Linux to be able to follow the lesson. Table of Contents # The fasta format The fastq format The sam/bam format The vcf format The gff format The fasta format # The fasta format was invented in 1988 and designed to represent nucleotide or peptide sequences. It originates from the FASTA software package, but is now a standard in the world of bioinformatics. The first line in a FASTA file starts with a \">\" (greater-than) symbol followed by the description or identifier of the sequence. Following the initial line (used for a unique description of the sequence) is the actual sequence itself in standard one-letter code. A few sample sequences: >KX580312.1 Homo sapiens truncated breast cancer 1 (BRCA1) gene, exon 15 and partial cds GTCATCCCCTTCTAAATGCCCATCATTAGATGATAGGTGGTACATGCACAGTTGCTCTGGGAGTCTTCAG AATAGAAACTACCCATCTCAAGAGGAGCTCATTAAGGTTGTTGATGTGGAGGAGTAACAGCTGGAAGAGT CTGGGCCACACGATTTGACGGAAACATCTTACTTGCCAAGGCAAGATCTAG >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE A fasta file can contain multiple sequence. Each sequence will be separated by their \"header\" line, starting by \">\". Example: >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE >3HHU_A Chain A, Human Heat-Shock Protein 90 (Hsp90) MPEETQTQDQPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKL DSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYS AYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKH SQFIGYPITLFVEK The fastq format # The fastq format is also a text based format to represent nucleotide sequences, but also contains the corresponding quality of each nucleotide. It is the standard for storing the output of high-throughput sequencing instruments such as the Illumina machines. A fastq file uses four lines per sequence: Line 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line). Line 2 is the raw sequence letters. Line 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again. Line 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence. An example sequence in fastq format: @SEQ_ID GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + !''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65 Quality # The quality, also called phred score, is the probability that the corresponding basecall is incorrect. Phred scores use a logarithmic scale, and are represented by ASCII characters, mapping to a quality usually going from 0 to 40. Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999% the sam/bam format # From Wikipedia : SAM (file format) is a text-based format for storing biological sequences aligned to a reference sequence developed by Heng Li. The acronym SAM stands for Sequence Alignment/Map. It is widely used for storing data, such as nucleotide sequences, generated by Next generation sequencing technologies and usually mapped to a reference. The SAM format consists of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file. SAM files can be analysed and edited with the software SAMtools. The SAM format has a really extensive and complex specification that you can find here . In brief it consists of a header section and reads (with other information) in tab delimited format. Example header section # @HD VN:1.0 SO:unsorted @SQ SN:O_volvulusOVOC_OM1a LN:2816604 @SQ SN:O_volvulusOVOC_OM1b LN:28345163 @SQ SN:O_volvulusOVOC_OM2 LN:25485961 Example read # M01137:130:00-A:17009:1352/14 * 0 0 * * 0 0 AGCAAAATACAACGATCTGGATGGTAGCATTAGCGATGCGACACTGCTTGAACCGTCAAAG FGGFGCFGFFGC8,,@D?E6EFCF,=AEFFGGDGGGADFGG@>FFEGGG:+<7D>AFCFGG YT:Z:UU the vcf format # The vcf format is also a text-based file format. VCF stands for Variant Call Format and is used to store gene sequence variations (SNVs, indels). The format has been developped for genotyping projects, and is the standard to represent variations in the genome of a species. A vcf is a tab-delimited file, described here . VCF Example # ##fileformat=VCFv4.0 ##fileDate=20110705 ##reference=1000GenomesPilot-NCBI37 ##phasing=partial ##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\"> ##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\"> ##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\"> ##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\"> ##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\"> ##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\"> ##FILTER=<ID=q10,Description=\"Quality below 10\"> ##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\"> ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\"> ##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\"> ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"> ##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\"> #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample1 Sample2 Sample3 2 4370 rs6057 G A 29 . NS=2;DP=13;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:52,51 1|0:48:8:51,51 1/1:43:5:.,. 2 7330 . T A 3 q10 NS=5;DP=12;AF=0.017 GT:GQ:DP:HQ 0|0:46:3:58,50 0|1:3:5:65,3 0/0:41:3 2 110696 rs6055 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:4 2 130237 . T . 47 . NS=2;DP=16;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:56,51 0/0:61:2 2 134567 microsat1 GTCT G,GTACT 50 PASS NS=2;DP=9;AA=G GT:GQ:DP 0/1:35:4 0/2:17:2 1/1:40:3 chr1 45796269 . G C chr1 45797505 . C G chr1 45798555 . T C chr1 45798901 . C T chr1 45805566 . G C chr2 47703379 . C T chr2 48010488 . G A chr2 48030838 . A T chr2 48032875 . CTAT - chr2 48032937 . T C chr2 48033273 . TTTTTGTTTTAATTCCT - chr2 48033551 . C G chr2 48033910 . A T chr2 215632048 . G T chr2 215632125 . TT - chr2 215632155 . T C chr2 215632192 . G A chr2 215632255 . CA TG chr2 215634055 . C T the gff format # The general feature format (gff) is another text file format, used for describing genes and other features of DNA, RNA and protein sequences. It is the standard for annotation of genomes. A gff file should contain 9 columns, described here Example gff # ##description: evidence-based annotation of the human genome (GRCh38), version 25 (Ensembl 85) ##provider: GENCODE ##contact: gencode-help@sanger.ac.uk ##format: gtf ##date: 2016-07-15 chr1 HAVANA gene 11869 14409 . + . gene_id \"ENSG00000223972.5\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\"; chr1 HAVANA transcript 11869 14409 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 11869 12227 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 1; exon_id \"ENSE00002234944.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 12613 12721 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 2; exon_id \"ENSE00003582793.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 13221 14409 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 3; exon_id \"ENSE00002312635.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";","title":"File formats"},{"location":"file_formats/#lecture-sequencing-technologies","text":"","title":"Lecture sequencing technologies"},{"location":"file_formats/#file-formats","text":"This lecture is aimed at making you discover the most popular file formats used in bioinformatics. You're expected to have basic working knowledge of Linux to be able to follow the lesson.","title":"File Formats"},{"location":"file_formats/#table-of-contents","text":"The fasta format The fastq format The sam/bam format The vcf format The gff format","title":"Table of Contents"},{"location":"file_formats/#the-fasta-format","text":"The fasta format was invented in 1988 and designed to represent nucleotide or peptide sequences. It originates from the FASTA software package, but is now a standard in the world of bioinformatics. The first line in a FASTA file starts with a \">\" (greater-than) symbol followed by the description or identifier of the sequence. Following the initial line (used for a unique description of the sequence) is the actual sequence itself in standard one-letter code. A few sample sequences: >KX580312.1 Homo sapiens truncated breast cancer 1 (BRCA1) gene, exon 15 and partial cds GTCATCCCCTTCTAAATGCCCATCATTAGATGATAGGTGGTACATGCACAGTTGCTCTGGGAGTCTTCAG AATAGAAACTACCCATCTCAAGAGGAGCTCATTAAGGTTGTTGATGTGGAGGAGTAACAGCTGGAAGAGT CTGGGCCACACGATTTGACGGAAACATCTTACTTGCCAAGGCAAGATCTAG >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE A fasta file can contain multiple sequence. Each sequence will be separated by their \"header\" line, starting by \">\". Example: >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE >3HHU_A Chain A, Human Heat-Shock Protein 90 (Hsp90) MPEETQTQDQPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKL DSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYS AYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKH SQFIGYPITLFVEK","title":"The fasta format"},{"location":"file_formats/#the-fastq-format","text":"The fastq format is also a text based format to represent nucleotide sequences, but also contains the corresponding quality of each nucleotide. It is the standard for storing the output of high-throughput sequencing instruments such as the Illumina machines. A fastq file uses four lines per sequence: Line 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line). Line 2 is the raw sequence letters. Line 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again. Line 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence. An example sequence in fastq format: @SEQ_ID GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + !''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65","title":"The fastq format"},{"location":"file_formats/#quality","text":"The quality, also called phred score, is the probability that the corresponding basecall is incorrect. Phred scores use a logarithmic scale, and are represented by ASCII characters, mapping to a quality usually going from 0 to 40. Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999%","title":"Quality"},{"location":"file_formats/#the-sambam-format","text":"From Wikipedia : SAM (file format) is a text-based format for storing biological sequences aligned to a reference sequence developed by Heng Li. The acronym SAM stands for Sequence Alignment/Map. It is widely used for storing data, such as nucleotide sequences, generated by Next generation sequencing technologies and usually mapped to a reference. The SAM format consists of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file. SAM files can be analysed and edited with the software SAMtools. The SAM format has a really extensive and complex specification that you can find here . In brief it consists of a header section and reads (with other information) in tab delimited format.","title":"the sam/bam format"},{"location":"file_formats/#example-header-section","text":"@HD VN:1.0 SO:unsorted @SQ SN:O_volvulusOVOC_OM1a LN:2816604 @SQ SN:O_volvulusOVOC_OM1b LN:28345163 @SQ SN:O_volvulusOVOC_OM2 LN:25485961","title":"Example header section"},{"location":"file_formats/#example-read","text":"M01137:130:00-A:17009:1352/14 * 0 0 * * 0 0 AGCAAAATACAACGATCTGGATGGTAGCATTAGCGATGCGACACTGCTTGAACCGTCAAAG FGGFGCFGFFGC8,,@D?E6EFCF,=AEFFGGDGGGADFGG@>FFEGGG:+<7D>AFCFGG YT:Z:UU","title":"Example read"},{"location":"file_formats/#the-vcf-format","text":"The vcf format is also a text-based file format. VCF stands for Variant Call Format and is used to store gene sequence variations (SNVs, indels). The format has been developped for genotyping projects, and is the standard to represent variations in the genome of a species. A vcf is a tab-delimited file, described here .","title":"the vcf format"},{"location":"file_formats/#vcf-example","text":"##fileformat=VCFv4.0 ##fileDate=20110705 ##reference=1000GenomesPilot-NCBI37 ##phasing=partial ##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\"> ##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\"> ##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\"> ##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\"> ##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\"> ##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\"> ##FILTER=<ID=q10,Description=\"Quality below 10\"> ##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\"> ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\"> ##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\"> ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"> ##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\"> #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample1 Sample2 Sample3 2 4370 rs6057 G A 29 . NS=2;DP=13;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:52,51 1|0:48:8:51,51 1/1:43:5:.,. 2 7330 . T A 3 q10 NS=5;DP=12;AF=0.017 GT:GQ:DP:HQ 0|0:46:3:58,50 0|1:3:5:65,3 0/0:41:3 2 110696 rs6055 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:4 2 130237 . T . 47 . NS=2;DP=16;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:56,51 0/0:61:2 2 134567 microsat1 GTCT G,GTACT 50 PASS NS=2;DP=9;AA=G GT:GQ:DP 0/1:35:4 0/2:17:2 1/1:40:3 chr1 45796269 . G C chr1 45797505 . C G chr1 45798555 . T C chr1 45798901 . C T chr1 45805566 . G C chr2 47703379 . C T chr2 48010488 . G A chr2 48030838 . A T chr2 48032875 . CTAT - chr2 48032937 . T C chr2 48033273 . TTTTTGTTTTAATTCCT - chr2 48033551 . C G chr2 48033910 . A T chr2 215632048 . G T chr2 215632125 . TT - chr2 215632155 . T C chr2 215632192 . G A chr2 215632255 . CA TG chr2 215634055 . C T","title":"VCF Example"},{"location":"file_formats/#the-gff-format","text":"The general feature format (gff) is another text file format, used for describing genes and other features of DNA, RNA and protein sequences. It is the standard for annotation of genomes. A gff file should contain 9 columns, described here","title":"the gff format"},{"location":"file_formats/#example-gff","text":"##description: evidence-based annotation of the human genome (GRCh38), version 25 (Ensembl 85) ##provider: GENCODE ##contact: gencode-help@sanger.ac.uk ##format: gtf ##date: 2016-07-15 chr1 HAVANA gene 11869 14409 . + . gene_id \"ENSG00000223972.5\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\"; chr1 HAVANA transcript 11869 14409 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 11869 12227 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 1; exon_id \"ENSE00002234944.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 12613 12721 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 2; exon_id \"ENSE00003582793.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 13221 14409 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 3; exon_id \"ENSE00002312635.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";","title":"Example gff"},{"location":"itrop/","text":"i-Trop cluster # Architecture of the i-Trop cluster # The IRD Bioinformatic Cluster is composed of a pool of machines reachable through a single entry point. The connections to the internal machines are managed by a master node that tries to ensure that proper balancing is made across the available nodes at a given moment: bioinfo-master.ird.fr The cluster is composed of: 1 master 3 nas servers for a 127To data storage 32 nodes servers : 8 nodes with 12 cores, 2 nodes with 16 cores, 4 nodes with 20 cores, 11 with 24 cores, 1 with 40 cores with RAM from 48Go to 1To and a GPU serveur with 8 RTX 2080 graphical cards. Here is the architecture Connect to the cluster via ssh # Open the terminal application and type the following command: ssh login@bioinfo-master.ird.fr with login: your cluster account First connection : Your password has to be changed at your first connection. \u201cMot de passe UNIX (actuel)\u201d: you are asked to type the password provided in the account creation email. Connect to a node in interactive mode and launch commands: # To connect to a node in interactive mode for X minutes, use the following command: srun -p short --time=X:00 --pty bash -i Then you can launch on this node without using the srun prefix srun Then type your new password twice. The session will be automatically closed. You will need to open a new session with your new password. Explore the cluster # The list of nodes in the cluster: sinfo -N nodes Choose a partition # short partition: short Jobs < 1 day normal partition: job of maximum 7 days long partition: 7 days< jobs < 45 days highmem partition: jobs with more memory needs highmemplus partition: jobs with more memory needs supermem partition: jobs with much more memory needs gpu partition: need of analyses on GPU cores Choose node5 ( highmemplus partition): # srun -p highmemplus --nodelist=node5 --pty bash -i Data location # All the data used in this training can be found in the scratch space of node5. ls /scratch/genesys_training/ Launching a program on the cluster # List the program already installed on the cluster: # module avail Display the description of a particular software # module whatis module_type/module_name/version with module_type: bioinfo or system with module_name: the name of the module. For example : for the version 1.7 of the bioinformatic software samtools: module whatis bioinfo/samtools/1.7 Load a particular software version # module load module_type/module_name/version with module_type: bioinfo or system with module_name: the name of the module. For example : for the version 1.7 of the bioinformatic software samtools: module load bioinfo/samtools/1.7 Unload a particular software version # module unload module_type/module_name/version with module_type: bioinfo or system with module_name: the name of the module. For example : for the version 1.7 of the bioinformatic software samtools: module unload bioinfo/samtools/1.7 Display all the modules loaded # module list unload all the modules loaded # module unload Launch a job # A job can be launched interactively using srun or via a script using sbatch Lauch an interactive job # Connect to a node in interactive mode and launch commands: # To connect to a node in interactive mode for X minutes , use the following command : srun -p short --time=X:00 --pty bash -i Then you can launch on this node without using the srun prefix srun Launching commands from the master # The following command allocate computing resources ( nodes, memory, cores) and immediately launch the command on each allocate resource. srun + command Example: module load bioinfo/FastQC/0.11.9 srun -p highmemplus --nodelist=node5 fastqc -t 2 K1_MinION.fastq.gz Launching jobs via a script # The batch mode allows to launch an analysis by following the steps described into a script Slurm allows to use different types of scripts such as bash, perl or python. Slurm allocates the desired computing resources and launch analyses on these resources in background. To be interpreted by Slurm, the script should contain a specific header with all the keyword #SBATCH to precise the Slurm options. . Slurm script example: #!/bin/bash ## Define job's name #SBATCH --job-name=flye ## Define the number of tasks #SBATCH --ntasks=1 ## Choose the node #SBATCH --nodelist=node11 ## Choose partition #SBATCH --partition=long #Define the timelimit #SBATCH --time=400:00:00 ## Define the number of cpus #SBATCH --cpus-per-task=8 ## Define the amount of ram per cpu #SBATCH --mem-per-cpu=6000 To launch an analysis use the following command: sbatch script.sh with script.sh the name of the script to use. Check job status # sacct -S 2020-11-2 -u galal --format=jobid,jobname,user,submit,start,end,state,NNodes,CPUTimeRAW,comment,Timelimit,TotalCPU,CPUTime,MaxDiskWrite,NodeList For checking all the jobs launched on the cluster: squeue For checking all the jobs of a certain user: squeue -u username","title":"iTrop computing cluster"},{"location":"itrop/#i-trop-cluster","text":"","title":"i-Trop cluster"},{"location":"itrop/#architecture-of-the-i-trop-cluster","text":"The IRD Bioinformatic Cluster is composed of a pool of machines reachable through a single entry point. The connections to the internal machines are managed by a master node that tries to ensure that proper balancing is made across the available nodes at a given moment: bioinfo-master.ird.fr The cluster is composed of: 1 master 3 nas servers for a 127To data storage 32 nodes servers : 8 nodes with 12 cores, 2 nodes with 16 cores, 4 nodes with 20 cores, 11 with 24 cores, 1 with 40 cores with RAM from 48Go to 1To and a GPU serveur with 8 RTX 2080 graphical cards. Here is the architecture","title":"Architecture of the i-Trop cluster"},{"location":"itrop/#connect-to-the-cluster-via-ssh","text":"Open the terminal application and type the following command: ssh login@bioinfo-master.ird.fr with login: your cluster account First connection : Your password has to be changed at your first connection. \u201cMot de passe UNIX (actuel)\u201d: you are asked to type the password provided in the account creation email.","title":"Connect to the cluster via ssh"},{"location":"itrop/#connect-to-a-node-in-interactive-mode-and-launch-commands","text":"To connect to a node in interactive mode for X minutes, use the following command: srun -p short --time=X:00 --pty bash -i Then you can launch on this node without using the srun prefix srun Then type your new password twice. The session will be automatically closed. You will need to open a new session with your new password.","title":"Connect to a node in interactive mode and launch commands:"},{"location":"itrop/#explore-the-cluster","text":"The list of nodes in the cluster: sinfo -N nodes","title":"Explore the cluster"},{"location":"itrop/#choose-a-partition","text":"short partition: short Jobs < 1 day normal partition: job of maximum 7 days long partition: 7 days< jobs < 45 days highmem partition: jobs with more memory needs highmemplus partition: jobs with more memory needs supermem partition: jobs with much more memory needs gpu partition: need of analyses on GPU cores","title":"Choose a partition"},{"location":"itrop/#choose-node5-highmemplus-partition","text":"srun -p highmemplus --nodelist=node5 --pty bash -i","title":"Choose node5 (highmemplus partition):"},{"location":"itrop/#data-location","text":"All the data used in this training can be found in the scratch space of node5. ls /scratch/genesys_training/","title":"Data location"},{"location":"itrop/#launching-a-program-on-the-cluster","text":"","title":"Launching a program on the cluster"},{"location":"itrop/#list-the-program-already-installed-on-the-cluster","text":"module avail","title":"List the program already installed on the cluster:"},{"location":"itrop/#display-the-description-of-a-particular-software","text":"module whatis module_type/module_name/version with module_type: bioinfo or system with module_name: the name of the module. For example : for the version 1.7 of the bioinformatic software samtools: module whatis bioinfo/samtools/1.7","title":"Display the description of a particular software"},{"location":"itrop/#load-a-particular-software-version","text":"module load module_type/module_name/version with module_type: bioinfo or system with module_name: the name of the module. For example : for the version 1.7 of the bioinformatic software samtools: module load bioinfo/samtools/1.7","title":"Load a particular software version"},{"location":"itrop/#unload-a-particular-software-version","text":"module unload module_type/module_name/version with module_type: bioinfo or system with module_name: the name of the module. For example : for the version 1.7 of the bioinformatic software samtools: module unload bioinfo/samtools/1.7","title":"Unload a particular software version"},{"location":"itrop/#display-all-the-modules-loaded","text":"module list","title":"Display all the modules loaded"},{"location":"itrop/#unload-all-the-modules-loaded","text":"module unload","title":"unload all the modules loaded"},{"location":"itrop/#launch-a-job","text":"A job can be launched interactively using srun or via a script using sbatch","title":"Launch a job"},{"location":"itrop/#lauch-an-interactive-job","text":"","title":"Lauch an interactive job"},{"location":"itrop/#connect-to-a-node-in-interactive-mode-and-launch-commands_1","text":"To connect to a node in interactive mode for X minutes , use the following command : srun -p short --time=X:00 --pty bash -i Then you can launch on this node without using the srun prefix srun","title":"Connect to a node in interactive mode and launch commands:"},{"location":"itrop/#launching-commands-from-the-master","text":"The following command allocate computing resources ( nodes, memory, cores) and immediately launch the command on each allocate resource. srun + command Example: module load bioinfo/FastQC/0.11.9 srun -p highmemplus --nodelist=node5 fastqc -t 2 K1_MinION.fastq.gz","title":"Launching commands from the master"},{"location":"itrop/#launching-jobs-via-a-script","text":"The batch mode allows to launch an analysis by following the steps described into a script Slurm allows to use different types of scripts such as bash, perl or python. Slurm allocates the desired computing resources and launch analyses on these resources in background. To be interpreted by Slurm, the script should contain a specific header with all the keyword #SBATCH to precise the Slurm options. . Slurm script example: #!/bin/bash ## Define job's name #SBATCH --job-name=flye ## Define the number of tasks #SBATCH --ntasks=1 ## Choose the node #SBATCH --nodelist=node11 ## Choose partition #SBATCH --partition=long #Define the timelimit #SBATCH --time=400:00:00 ## Define the number of cpus #SBATCH --cpus-per-task=8 ## Define the amount of ram per cpu #SBATCH --mem-per-cpu=6000 To launch an analysis use the following command: sbatch script.sh with script.sh the name of the script to use.","title":"Launching jobs via a script"},{"location":"itrop/#check-job-status","text":"sacct -S 2020-11-2 -u galal --format=jobid,jobname,user,submit,start,end,state,NNodes,CPUTimeRAW,comment,Timelimit,TotalCPU,CPUTime,MaxDiskWrite,NodeList For checking all the jobs launched on the cluster: squeue For checking all the jobs of a certain user: squeue -u username","title":"Check job status"},{"location":"long_reads/","text":"DE NOVO GENOME ASSEMBLY USING LONG READS # Basecalling # Base calling is the process of translating the electronic raw signal (fast5 format) of the sequencer into bases (fastq format). environment : the i-Trop computing cluster. software : guppy input data : /scratch/genesys_training/files/data/fast5/ link to the tutorial Quality assessment # Verify the quality of reads within nanopore fastq files. environment : the i-Trop computing cluster. software : fastqc input data : /scratch/genesys_training/files/data/lr_fastq/all_guppy.fastq link to the tutorial Read filtering, trimming and adapter removal # Verify the quality of reads within nanopore fastq files. environment : the i-Trop computing cluster. software : porechop; nanofilt input data : /scratch/genesys_training/files/data/lr_fastq/all_guppy.fastq link to the tutorial Genome assembly # \"De novo\" assembly of long reads environment : the i-Trop computing cluster. software : flye input data : Klebsiella pneumoniae nanopore sequences link to input files link to the tutorial","title":"Long reads"},{"location":"long_reads/#de-novo-genome-assembly-using-long-reads","text":"","title":"DE NOVO GENOME ASSEMBLY USING LONG READS"},{"location":"long_reads/#basecalling","text":"Base calling is the process of translating the electronic raw signal (fast5 format) of the sequencer into bases (fastq format). environment : the i-Trop computing cluster. software : guppy input data : /scratch/genesys_training/files/data/fast5/ link to the tutorial","title":"Basecalling"},{"location":"long_reads/#quality-assessment","text":"Verify the quality of reads within nanopore fastq files. environment : the i-Trop computing cluster. software : fastqc input data : /scratch/genesys_training/files/data/lr_fastq/all_guppy.fastq link to the tutorial","title":"Quality assessment"},{"location":"long_reads/#read-filtering-trimming-and-adapter-removal","text":"Verify the quality of reads within nanopore fastq files. environment : the i-Trop computing cluster. software : porechop; nanofilt input data : /scratch/genesys_training/files/data/lr_fastq/all_guppy.fastq link to the tutorial","title":"Read filtering, trimming and adapter removal"},{"location":"long_reads/#genome-assembly","text":"\"De novo\" assembly of long reads environment : the i-Trop computing cluster. software : flye input data : Klebsiella pneumoniae nanopore sequences link to input files link to the tutorial","title":"Genome assembly"},{"location":"metavir/","text":"Viral Metagenome from a dolphin sample: hunting for a disease causing virus # In this tutorial you will learn how to investigate metagenomics data and retrieve draft genome from an assembled metagenome. We will use a real dataset published in 2017 in a study in dolphins, where fecal samples where prepared for viral metagenomics study. The dolphin had a self-limiting gastroenteritis of suspected viral origin. Getting the Data # First, create an appropriate directory to put the data, within your directory for the training: mkdir -p dolphin/data cd dolphin/data You can download them from here: wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR193/ERR1938563/Dol1_S19_L001_R1_001.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR193/ERR1938563/Dol1_S19_L001_R2_001.fastq.gz Alternatively, your instructor will let you know where to get the dataset from. You should get 2 compressed files: Dol1_S19_L001_R1_001.fastq.gz Dol1_S19_L001_R2_001.fastq.gz Quality Control # We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --pty bash -i We will use FastQC to check the quality of our data, as well as fastp for trimming the bad quality part of the reads. If you need a refresher on how and why to check the quality of sequence data, please check the Quality Control and Trimming tutorial mkdir -p dolphin/results cd dolphin/results ln -s ../data/Dol1* . module load bioinfo/FastQC/0.11.9 fastqc Dol1_*.fastq.gz Question What is the average read length? The average quality? Question Compared to single genome sequencing, which graphs differ? Quality control with Fastp # We will removing the adapters and trim by quality. Now we run fastp our read files module load bioinfo/fastp/0.20.1 fastp -i Dol1_S19_L001_R1_001.fastq.gz -o Dol1_trimmed_R1.fastq \\ -I Dol1_S19_L001_R2_001.fastq.gz -O Dol1_trimmed_R2.fastq \\ --detect_adapter_for_pe --length_required 30 \\ --cut_front --cut_tail --cut_mean_quality 10 Check the html report produced. Question How many reads were trimmed? Removing the host sequences by mapping/aligning on the dolphin genome # For this we will use Bowtie2. We have downloaded the genome of Tursiops truncatus from Ensembl (fasta file). Then we have run the following command to produce the indexes of the dolphin genome for Bowtie2 (do not run it, we have pre-calculated the results for you): bowtie2-build Tursiops_truncatus.turTru1.dna.toplevel.fa Tursiops_truncatus Because this step takes a while, we have precomputed the index files, you can get them from here: cd dolphin/data cp /scratch/genesys_training/files/dolphin/Tursiops_truncatus*.bt2 . Now we are ready to map our sequencing reads on the dolphin genome: cd ../results module load bioinfo/bowtie2/2.3.4.1 bowtie2 -x ../data/Tursiops_truncatus \\ -1 Dol1_trimmed_R1.fastq -2 Dol1_trimmed_R2.fastq \\ -S Dol1_map.sam --un-conc Dol1_reads_unmapped.fastq --threads 2 Question How many reads have mapped on the dolphin genome? Taxonomic classification of the trimmed reads with Kraken2 # We will use Kraken2 for the classification of the unmapped reads. module load bioinfo/kraken2/2.1.1 mkdir Dol1_reads_unmapped_Kn2nt kraken2 --db /data/projects/banks/kraken2/nt/21-09/nt/ --memory-mapping --threads 4 --output Dol1_reads_unmapped_Kn2nt/Dol1_reads_unmapped_kn2_nt-res.txt --report Dol1_reads_unmapped_Kn2nt/Dol1_reads_unmapped_kn2_nt-report.txt --report-minimizer-data --paired Dol1_reads_unmapped.1.fastq Dol1_reads_unmapped.2.fastq This command can take very long, therefore we recommend you to prepare a more generic script for running Kraken, with the possibility to run on paired reads or on a single contigs fasta file. Follow the example below to prepare a kraken2_nt.sh script in your scripts directory. #!/bin/bash ## SLURM CONFIG ## #SBATCH --job-name=kn2nt #SBATCH --output=%x.%j.out #SBATCH -c 4 #SBATCH --time=96:00:00 #SBATCH -p SELECTED_PARTITION #SBATCH --mail-type=FAIL,END #SBATCH --mem-per-cpu=4G ### HELP=\" USAGE: kraken2_nt.sh reads_file1.fastq [reads_file2.fastq] \" # If we didn't get any arguments, print help and exit if [[ $# < 1 ]] then echo \"$HELP\" exit 0 fi # If we have a help flag anywhere among the arguments for arg in $@ do if [ \"$arg\" == \"-h\" ] || [ \"$arg\" == \"--help\" ] then echo \"$HELP\" fi done ##### KRAKEN VARIABLES #### PATH_DB=\"/data/projects/banks/kraken2/nt/21-09/nt/\" QUERY_FILE=$1 PREFIX=$(echo $1 | cut -f1 -d.) DATA_DIR=`pwd` OUT_DIR=\"${DATA_DIR}/${PREFIX}_Kn2nt\" mkdir $OUT_DIR module load bioinfo/kraken2/2.1.1 if [[ $# > 1 ]] then # Taxonomic classification with Kraken2 on nt db for paired end reads QUERY_FILE2=$2 kraken2 --db ${PATH_DB} --memory-mapping --threads 4 --output ${OUT_DIR}/${PREFIX}_kn2_nt-res.txt --report ${OUT_DIR}/${PREFIX}_kn2_nt-report.txt --report-minimizer-data --paired ${QUERY_FILE} ${QUERY_FILE2} else # one single input file (e.g: scaffolds.fasta) kraken2 --db ${PATH_DB} --memory-mapping --threads 4 --output ${OUT_DIR}/${PREFIX}_kn2_nt-res.txt --report ${OUT_DIR}/${PREFIX}_kn2_nt-report.txt --report-minimizer-data ${QUERY_FILE} fi Once the script is ready, you can run it on the reads as follow: sbatch ../../scripts/kraken2_nt.sh Dol1_reads_unmapped.1.fastq Dol1_reads_unmapped.2.fastq Question Inspect the 2 output files from Kraken and comment. Assembly # Megahit will be used for the de novo assembly of the metagenome. module load bioinfo/MEGAHIT/1.2.9 megahit -1 Dol1_reads_unmapped.1.fastq -2 Dol1_reads_unmapped.2.fastq -o Dol1_assembly The resulting assembly can be found under assembly/final.contigs.fa . Question How many contigs does this assembly contain? Is there any long contig? Tip Use the command grep with the symbol > to visualise the fasta sequences headers and count them Taxonomic classification of contigs # We will use Kraken again with the same nt database for the classification of the produced contigs. # in the results directory, link the contigs file ln -s Dol1_assembly/final.contigs.fa Dol1_assembly_contigs.fa # then run you previously made Kraken script using sbatch sbatch ../../scripts/kraken2_nt.sh Dol1_assembly_contigs.fa Question Does the classification of contigs produce different results than the classification of reads? Extraction of the contig of interest # Let's go for a little practice of your Unix skills! Question Find a way to to find the longest contig. They look like this: >k141_1 flag=1 multi=1.0000 len=301 >k141_2 flag=1 multi=1.0000 len=303 hint 1 : all lines containing '>' hint 2 : use grep and sed Once we have this file, we want to sort all the sequences headers by the sequence length (len=X): cat Dol1_assembly_contigs.fa | grep \">\" | sed s/len=// | sort -k4n | tail -1 Question What is the size of the longest contig? Now that you have identified the sequence header or id of the longest contig, you want to save it to a fasta file. grep -i '>k141_XXX' -A 1 Dol1_assembly_contigs.fa > longest_contig.fasta Note You need to replace the XXX by the correct header. The option -A 1 of grep allows to print 1 line additionally to the matching line (which enables to print the full sequence that corresponds to one line). Test with -A 2 (without the redirection to longest_contig.fasta) to see what happens. Now that you have identified the longest contig, you will check in Kraken results what was the taxon assigned to this contig. Have a look at the file Dol1_assembly_contigs_kn2_nt-res.txt . It is structured in 5 columns: classification status (C/U), sequence id, assigned TaxID, sequence length, k-mers classification. For more info about Kraken2, check the manual Question Identify the TaxID of the longest contig and search on NCBI Taxonomy database to which species it corresponds to. Note Note that there is an easy way to extract all the sequences classified at a certain Taxon (using the NCBI TaxID), including or not the sequences classified at \"children taxa\" of this. A useful Python script is available through KrakenTools with a few other tools around Kraken. If you have the time, do not hesitate to ask the teacher to help you installing and using it. Pavian # We will use Pavian for visualising Kraken2 results for reads and contigs. First, copy all the Kraken reports on your computer. Go to Rstudio and follow the instructions of Pavian for running the app. The teacher will guide you for this step. if (!require(remotes)) { install.packages(\"remotes\") } remotes::install_github(\"fbreitwieser/pavian\") To run Pavian from R, type: pavian::runApp(port=5000) Genome annotation of the contig of interest # Once the contig to annotate is extracted and saved in the file longest_contig.fasta, we will use Prokka to detect ORFs (Open Reading Frames) in order to predict genes and their resulting proteins. First, go to Uniprot database and retrieve a set of protein sequences belonging to adenoviruses. Save the file as adenovirus.faa and copy it in your results directory. module load bioinfo/prokka/1.14.6 prokka --outdir annotation --kingdom Viruses \\ --proteins adenovirus.faa longest_contig.fasta Question How many genes and proteins were predicted? Visualization and manual curation. # If there is some time left, you can visualise the produced annotation (gff file) in Ugene or Artemis for example. Go further with proteins functions # For the predicted proteins that are left \"hypotetical\", you can try running Interproscan on them to get more information on domains and motifs.","title":"Viral Metagenomics"},{"location":"metavir/#viral-metagenome-from-a-dolphin-sample-hunting-for-a-disease-causing-virus","text":"In this tutorial you will learn how to investigate metagenomics data and retrieve draft genome from an assembled metagenome. We will use a real dataset published in 2017 in a study in dolphins, where fecal samples where prepared for viral metagenomics study. The dolphin had a self-limiting gastroenteritis of suspected viral origin.","title":"Viral Metagenome from a dolphin sample: hunting for a disease causing virus"},{"location":"metavir/#getting-the-data","text":"First, create an appropriate directory to put the data, within your directory for the training: mkdir -p dolphin/data cd dolphin/data You can download them from here: wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR193/ERR1938563/Dol1_S19_L001_R1_001.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR193/ERR1938563/Dol1_S19_L001_R2_001.fastq.gz Alternatively, your instructor will let you know where to get the dataset from. You should get 2 compressed files: Dol1_S19_L001_R1_001.fastq.gz Dol1_S19_L001_R2_001.fastq.gz","title":"Getting the Data"},{"location":"metavir/#quality-control","text":"We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --pty bash -i We will use FastQC to check the quality of our data, as well as fastp for trimming the bad quality part of the reads. If you need a refresher on how and why to check the quality of sequence data, please check the Quality Control and Trimming tutorial mkdir -p dolphin/results cd dolphin/results ln -s ../data/Dol1* . module load bioinfo/FastQC/0.11.9 fastqc Dol1_*.fastq.gz Question What is the average read length? The average quality? Question Compared to single genome sequencing, which graphs differ?","title":"Quality Control"},{"location":"metavir/#quality-control-with-fastp","text":"We will removing the adapters and trim by quality. Now we run fastp our read files module load bioinfo/fastp/0.20.1 fastp -i Dol1_S19_L001_R1_001.fastq.gz -o Dol1_trimmed_R1.fastq \\ -I Dol1_S19_L001_R2_001.fastq.gz -O Dol1_trimmed_R2.fastq \\ --detect_adapter_for_pe --length_required 30 \\ --cut_front --cut_tail --cut_mean_quality 10 Check the html report produced. Question How many reads were trimmed?","title":"Quality control with Fastp"},{"location":"metavir/#removing-the-host-sequences-by-mappingaligning-on-the-dolphin-genome","text":"For this we will use Bowtie2. We have downloaded the genome of Tursiops truncatus from Ensembl (fasta file). Then we have run the following command to produce the indexes of the dolphin genome for Bowtie2 (do not run it, we have pre-calculated the results for you): bowtie2-build Tursiops_truncatus.turTru1.dna.toplevel.fa Tursiops_truncatus Because this step takes a while, we have precomputed the index files, you can get them from here: cd dolphin/data cp /scratch/genesys_training/files/dolphin/Tursiops_truncatus*.bt2 . Now we are ready to map our sequencing reads on the dolphin genome: cd ../results module load bioinfo/bowtie2/2.3.4.1 bowtie2 -x ../data/Tursiops_truncatus \\ -1 Dol1_trimmed_R1.fastq -2 Dol1_trimmed_R2.fastq \\ -S Dol1_map.sam --un-conc Dol1_reads_unmapped.fastq --threads 2 Question How many reads have mapped on the dolphin genome?","title":"Removing the host sequences by mapping/aligning on the dolphin genome"},{"location":"metavir/#taxonomic-classification-of-the-trimmed-reads-with-kraken2","text":"We will use Kraken2 for the classification of the unmapped reads. module load bioinfo/kraken2/2.1.1 mkdir Dol1_reads_unmapped_Kn2nt kraken2 --db /data/projects/banks/kraken2/nt/21-09/nt/ --memory-mapping --threads 4 --output Dol1_reads_unmapped_Kn2nt/Dol1_reads_unmapped_kn2_nt-res.txt --report Dol1_reads_unmapped_Kn2nt/Dol1_reads_unmapped_kn2_nt-report.txt --report-minimizer-data --paired Dol1_reads_unmapped.1.fastq Dol1_reads_unmapped.2.fastq This command can take very long, therefore we recommend you to prepare a more generic script for running Kraken, with the possibility to run on paired reads or on a single contigs fasta file. Follow the example below to prepare a kraken2_nt.sh script in your scripts directory. #!/bin/bash ## SLURM CONFIG ## #SBATCH --job-name=kn2nt #SBATCH --output=%x.%j.out #SBATCH -c 4 #SBATCH --time=96:00:00 #SBATCH -p SELECTED_PARTITION #SBATCH --mail-type=FAIL,END #SBATCH --mem-per-cpu=4G ### HELP=\" USAGE: kraken2_nt.sh reads_file1.fastq [reads_file2.fastq] \" # If we didn't get any arguments, print help and exit if [[ $# < 1 ]] then echo \"$HELP\" exit 0 fi # If we have a help flag anywhere among the arguments for arg in $@ do if [ \"$arg\" == \"-h\" ] || [ \"$arg\" == \"--help\" ] then echo \"$HELP\" fi done ##### KRAKEN VARIABLES #### PATH_DB=\"/data/projects/banks/kraken2/nt/21-09/nt/\" QUERY_FILE=$1 PREFIX=$(echo $1 | cut -f1 -d.) DATA_DIR=`pwd` OUT_DIR=\"${DATA_DIR}/${PREFIX}_Kn2nt\" mkdir $OUT_DIR module load bioinfo/kraken2/2.1.1 if [[ $# > 1 ]] then # Taxonomic classification with Kraken2 on nt db for paired end reads QUERY_FILE2=$2 kraken2 --db ${PATH_DB} --memory-mapping --threads 4 --output ${OUT_DIR}/${PREFIX}_kn2_nt-res.txt --report ${OUT_DIR}/${PREFIX}_kn2_nt-report.txt --report-minimizer-data --paired ${QUERY_FILE} ${QUERY_FILE2} else # one single input file (e.g: scaffolds.fasta) kraken2 --db ${PATH_DB} --memory-mapping --threads 4 --output ${OUT_DIR}/${PREFIX}_kn2_nt-res.txt --report ${OUT_DIR}/${PREFIX}_kn2_nt-report.txt --report-minimizer-data ${QUERY_FILE} fi Once the script is ready, you can run it on the reads as follow: sbatch ../../scripts/kraken2_nt.sh Dol1_reads_unmapped.1.fastq Dol1_reads_unmapped.2.fastq Question Inspect the 2 output files from Kraken and comment.","title":"Taxonomic classification of the trimmed reads with Kraken2"},{"location":"metavir/#assembly","text":"Megahit will be used for the de novo assembly of the metagenome. module load bioinfo/MEGAHIT/1.2.9 megahit -1 Dol1_reads_unmapped.1.fastq -2 Dol1_reads_unmapped.2.fastq -o Dol1_assembly The resulting assembly can be found under assembly/final.contigs.fa . Question How many contigs does this assembly contain? Is there any long contig? Tip Use the command grep with the symbol > to visualise the fasta sequences headers and count them","title":"Assembly"},{"location":"metavir/#taxonomic-classification-of-contigs","text":"We will use Kraken again with the same nt database for the classification of the produced contigs. # in the results directory, link the contigs file ln -s Dol1_assembly/final.contigs.fa Dol1_assembly_contigs.fa # then run you previously made Kraken script using sbatch sbatch ../../scripts/kraken2_nt.sh Dol1_assembly_contigs.fa Question Does the classification of contigs produce different results than the classification of reads?","title":"Taxonomic classification of contigs"},{"location":"metavir/#extraction-of-the-contig-of-interest","text":"Let's go for a little practice of your Unix skills! Question Find a way to to find the longest contig. They look like this: >k141_1 flag=1 multi=1.0000 len=301 >k141_2 flag=1 multi=1.0000 len=303 hint 1 : all lines containing '>' hint 2 : use grep and sed Once we have this file, we want to sort all the sequences headers by the sequence length (len=X): cat Dol1_assembly_contigs.fa | grep \">\" | sed s/len=// | sort -k4n | tail -1 Question What is the size of the longest contig? Now that you have identified the sequence header or id of the longest contig, you want to save it to a fasta file. grep -i '>k141_XXX' -A 1 Dol1_assembly_contigs.fa > longest_contig.fasta Note You need to replace the XXX by the correct header. The option -A 1 of grep allows to print 1 line additionally to the matching line (which enables to print the full sequence that corresponds to one line). Test with -A 2 (without the redirection to longest_contig.fasta) to see what happens. Now that you have identified the longest contig, you will check in Kraken results what was the taxon assigned to this contig. Have a look at the file Dol1_assembly_contigs_kn2_nt-res.txt . It is structured in 5 columns: classification status (C/U), sequence id, assigned TaxID, sequence length, k-mers classification. For more info about Kraken2, check the manual Question Identify the TaxID of the longest contig and search on NCBI Taxonomy database to which species it corresponds to. Note Note that there is an easy way to extract all the sequences classified at a certain Taxon (using the NCBI TaxID), including or not the sequences classified at \"children taxa\" of this. A useful Python script is available through KrakenTools with a few other tools around Kraken. If you have the time, do not hesitate to ask the teacher to help you installing and using it.","title":"Extraction of the contig of interest"},{"location":"metavir/#pavian","text":"We will use Pavian for visualising Kraken2 results for reads and contigs. First, copy all the Kraken reports on your computer. Go to Rstudio and follow the instructions of Pavian for running the app. The teacher will guide you for this step. if (!require(remotes)) { install.packages(\"remotes\") } remotes::install_github(\"fbreitwieser/pavian\") To run Pavian from R, type: pavian::runApp(port=5000)","title":"Pavian"},{"location":"metavir/#genome-annotation-of-the-contig-of-interest","text":"Once the contig to annotate is extracted and saved in the file longest_contig.fasta, we will use Prokka to detect ORFs (Open Reading Frames) in order to predict genes and their resulting proteins. First, go to Uniprot database and retrieve a set of protein sequences belonging to adenoviruses. Save the file as adenovirus.faa and copy it in your results directory. module load bioinfo/prokka/1.14.6 prokka --outdir annotation --kingdom Viruses \\ --proteins adenovirus.faa longest_contig.fasta Question How many genes and proteins were predicted?","title":"Genome annotation of the contig of interest"},{"location":"metavir/#visualization-and-manual-curation","text":"If there is some time left, you can visualise the produced annotation (gff file) in Ugene or Artemis for example.","title":"Visualization and manual curation."},{"location":"metavir/#go-further-with-proteins-functions","text":"For the predicted proteins that are left \"hypotetical\", you can try running Interproscan on them to get more information on domains and motifs.","title":"Go further with proteins functions"},{"location":"mkdocs_usage/","text":"Welcome to MkDocs # For full documentation visit mkdocs.org . Commands # mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs gh-deploy - Deploy the doc on GitHub pages mkdocs -h - Print help message and exit. Project layout # mkdocs.yml # The configuration file. docs/ usage.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Mkdocs usage"},{"location":"mkdocs_usage/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"mkdocs_usage/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs gh-deploy - Deploy the doc on GitHub pages mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"mkdocs_usage/#project-layout","text":"mkdocs.yml # The configuration file. docs/ usage.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"mlst/","text":"Multi-Locus Sequence Typing # In this practical we will perform the genome typing by MLST analysis on the assembly of Klebsiella pneumoniae , using the contigs/scaffolds produced by the hybrid assembly in a previous tutorial. Prepare our computing environment # We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i You are now on a computing node, with computing 2 cpus reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit Run the MLST program # We should now identify our fasta file containing the contigs from the Unicycler hybrid assembly, produced from long and short reads. This will be the input file for mlst. cd results ls -l Now we will run the MLST tool: module load system/singularity/3.6.0 singularity run /path/to/mlst_singularity_container ... mlst K2_unicycler_scaffolds.fasta > K2_unicycler_scaffolds_mlst.tsv Tip Ask the teacher for the MLST container path. Once finished, you can exit the container by typing exit or press CTRL+D Question Check the output file. What is the sequence type of this strain? Question If the (hybrid) assemblies of the other Klebsiella pneumoniae strains are provided, try to make a loop to run the MLST analysis on all.","title":"MLST"},{"location":"mlst/#multi-locus-sequence-typing","text":"In this practical we will perform the genome typing by MLST analysis on the assembly of Klebsiella pneumoniae , using the contigs/scaffolds produced by the hybrid assembly in a previous tutorial.","title":"Multi-Locus Sequence Typing"},{"location":"mlst/#prepare-our-computing-environment","text":"We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i You are now on a computing node, with computing 2 cpus reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit","title":"Prepare our computing environment"},{"location":"mlst/#run-the-mlst-program","text":"We should now identify our fasta file containing the contigs from the Unicycler hybrid assembly, produced from long and short reads. This will be the input file for mlst. cd results ls -l Now we will run the MLST tool: module load system/singularity/3.6.0 singularity run /path/to/mlst_singularity_container ... mlst K2_unicycler_scaffolds.fasta > K2_unicycler_scaffolds_mlst.tsv Tip Ask the teacher for the MLST container path. Once finished, you can exit the container by typing exit or press CTRL+D Question Check the output file. What is the sequence type of this strain? Question If the (hybrid) assemblies of the other Klebsiella pneumoniae strains are provided, try to make a loop to run the MLST analysis on all.","title":"Run the MLST program"},{"location":"pan_genome/","text":"Pan-Genome Analysis # In this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes. The pangenome analysis will be based on the genome annotations made in the previous step, that we obtained thanks to Prokka. The presence/absence of genes will be used by Roary for determining genes belonging to the core genome, and the genes being accessory among the pangenome (not present in all strains). We will use Roary for performing this pangenome analysis. Prepare our computing environment # We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i You are now on a computing node, with computing 2 cpus reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit Input data # Prokka requires assembled contigs. You can prepare you working directory for this annotation tutorial. cd results/annotation ls -l Put all the .gff files in the same folder (e.g., ./gff ) for running Roary mkdir gff cp K*prokka/*.gff gff/ Pan-genome analysis # We will now run Roary from the gff directory module load bioinfo/roary/3.12.0 cd gff roary -p 4 -o 5_Kp_roary_clust -f 5_Kp_roary -e -n -r -v *.gff This can take 15-20 min. Roary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the summary_statistics.txt file. Additionally, Roary produces a gene_presence_absence.csv file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not. Inspect the results files produced cd 5_Kp_roary ls -l Question How many genes are found to be in the core genome? How many are shared by several strains but not all (shell)? How many are found in only one strain (cloud genes)? Plotting the results # Roary comes with a python script that allows you to generate a few plots to graphically assess your analysis output. First, we need to generate a tree file from the alignment generated by Roary: FastTree -nt -gtr core_gene_alignment.aln > my_tree.newick Then we can plot the Roary results with roary_plots.py , a community contributed python script to visualise roary results: wget https://raw.githubusercontent.com/sanger-pathogens/Roary/master/contrib/roary_plots/roary_plots.py module load system/python/3.8.12 pip install seaborn python roary_plots.py python roary_plots.py my_tree.newick gene_presence_absence.csv then look at the 3 /png files that have been generated","title":"Pangenome"},{"location":"pan_genome/#pan-genome-analysis","text":"In this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes. The pangenome analysis will be based on the genome annotations made in the previous step, that we obtained thanks to Prokka. The presence/absence of genes will be used by Roary for determining genes belonging to the core genome, and the genes being accessory among the pangenome (not present in all strains). We will use Roary for performing this pangenome analysis.","title":"Pan-Genome Analysis"},{"location":"pan_genome/#prepare-our-computing-environment","text":"We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i You are now on a computing node, with computing 2 cpus reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit","title":"Prepare our computing environment"},{"location":"pan_genome/#input-data","text":"Prokka requires assembled contigs. You can prepare you working directory for this annotation tutorial. cd results/annotation ls -l Put all the .gff files in the same folder (e.g., ./gff ) for running Roary mkdir gff cp K*prokka/*.gff gff/","title":"Input data"},{"location":"pan_genome/#pan-genome-analysis_1","text":"We will now run Roary from the gff directory module load bioinfo/roary/3.12.0 cd gff roary -p 4 -o 5_Kp_roary_clust -f 5_Kp_roary -e -n -r -v *.gff This can take 15-20 min. Roary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the summary_statistics.txt file. Additionally, Roary produces a gene_presence_absence.csv file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not. Inspect the results files produced cd 5_Kp_roary ls -l Question How many genes are found to be in the core genome? How many are shared by several strains but not all (shell)? How many are found in only one strain (cloud genes)?","title":"Pan-genome analysis"},{"location":"pan_genome/#plotting-the-results","text":"Roary comes with a python script that allows you to generate a few plots to graphically assess your analysis output. First, we need to generate a tree file from the alignment generated by Roary: FastTree -nt -gtr core_gene_alignment.aln > my_tree.newick Then we can plot the Roary results with roary_plots.py , a community contributed python script to visualise roary results: wget https://raw.githubusercontent.com/sanger-pathogens/Roary/master/contrib/roary_plots/roary_plots.py module load system/python/3.8.12 pip install seaborn python roary_plots.py python roary_plots.py my_tree.newick gene_presence_absence.csv then look at the 3 /png files that have been generated","title":"Plotting the results"},{"location":"plasmids/","text":"Plasmids detection # We will now search for plasmids features in the assembled scaffolds/contigs to detect which ones could have originated from a plasmid. For this, we will use 2 different tools: PlasmidFinder , developed at the Center for Genomic Epidemiology , like many other tools for bacterial genomics analysis. It searched for known plasmids and provide the replicon type. Platon , a tool that detects plasmid-borne contigs within bacterial draft (meta) genomes assemblies. Therefore, Platon analyzes the distribution bias of protein-coding gene families among chromosomes and plasmids. This analysis is complemented by comprehensive contig characterizations followed by heuristic filters. Platon depends on a custom database based on MPS, RDS, RefSeq Plasmid database, PlasmidFinder db as well as manually curated MOB HMM models from MOBscan, custom conjugation and replication HMM models and oriT sequences from MOB-suite. Prepare our computing environment # We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i You are now on a computing node, with computing 2 cpus reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit Practical # First, you need to identify in your results directory the fasta file corresponding to the unicycler hybrid assembly. cd results ls -l PlasmidFinder # We will use the PlasmidFinder container that has all tools needed for running PlasmidFinder module load system/singularity/3.6.0 singularity shell path/to/plasmidfiner_container mkdir K2_plasmidfinder plasmidfinder.py -i K2_unicycler_scaffolds.fasta -o K2_plasmidfinder -p /path/to/PF_DB -mp blastn -x Tip Ask the teacher for the PlasmidFinder database path and for the PlasmidFinder container path. Once finished, you can exit the container by typing exit or press CTRL+D Question Inspect the output files within the plasmidfinder output directory and comment. How many potential contigs of plasmidic origin were identified ? Platon # We will use a second tool for detecting sequences of plasmidic origin: Platon module load system/singularity/3.6.0 singularity shell path/to/platon_container platon --db path/to/databases/platon/db/ -t 4 -o K2_unicycler_platon_accu K2_unicycler_scaffolds.fasta Tip Ask the teacher for the Platon database path and for the Platon container path. Question Inspect the output files within the Platon output directory and comment. Compare with results obtained with PlasmidFinder Note You can put the Platon json file in Json Editor online for visualising the hits Question Try to run the tool on all other Unicycler assemblies by using a loop","title":"Plasmids"},{"location":"plasmids/#plasmids-detection","text":"We will now search for plasmids features in the assembled scaffolds/contigs to detect which ones could have originated from a plasmid. For this, we will use 2 different tools: PlasmidFinder , developed at the Center for Genomic Epidemiology , like many other tools for bacterial genomics analysis. It searched for known plasmids and provide the replicon type. Platon , a tool that detects plasmid-borne contigs within bacterial draft (meta) genomes assemblies. Therefore, Platon analyzes the distribution bias of protein-coding gene families among chromosomes and plasmids. This analysis is complemented by comprehensive contig characterizations followed by heuristic filters. Platon depends on a custom database based on MPS, RDS, RefSeq Plasmid database, PlasmidFinder db as well as manually curated MOB HMM models from MOBscan, custom conjugation and replication HMM models and oriT sequences from MOB-suite.","title":"Plasmids detection"},{"location":"plasmids/#prepare-our-computing-environment","text":"We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --cpus-per-task 2 --pty bash -i You are now on a computing node, with computing 2 cpus reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit","title":"Prepare our computing environment"},{"location":"plasmids/#practical","text":"First, you need to identify in your results directory the fasta file corresponding to the unicycler hybrid assembly. cd results ls -l","title":"Practical"},{"location":"plasmids/#plasmidfinder","text":"We will use the PlasmidFinder container that has all tools needed for running PlasmidFinder module load system/singularity/3.6.0 singularity shell path/to/plasmidfiner_container mkdir K2_plasmidfinder plasmidfinder.py -i K2_unicycler_scaffolds.fasta -o K2_plasmidfinder -p /path/to/PF_DB -mp blastn -x Tip Ask the teacher for the PlasmidFinder database path and for the PlasmidFinder container path. Once finished, you can exit the container by typing exit or press CTRL+D Question Inspect the output files within the plasmidfinder output directory and comment. How many potential contigs of plasmidic origin were identified ?","title":"PlasmidFinder"},{"location":"plasmids/#platon","text":"We will use a second tool for detecting sequences of plasmidic origin: Platon module load system/singularity/3.6.0 singularity shell path/to/platon_container platon --db path/to/databases/platon/db/ -t 4 -o K2_unicycler_platon_accu K2_unicycler_scaffolds.fasta Tip Ask the teacher for the Platon database path and for the Platon container path. Question Inspect the output files within the Platon output directory and comment. Compare with results obtained with PlasmidFinder Note You can put the Platon json file in Json Editor online for visualising the hits Question Try to run the tool on all other Unicycler assemblies by using a loop","title":"Platon"},{"location":"project_organisation/","text":"Project organization and management # Most of the the project organization material can be found at https://software-carpentry.org and http://www.datacarpentry.org Many thanks to them for existing! Structure or architecture of a data science project # Some good practice when you will organise your project directory on the server, on the cloud or any other machine where you will compute: Create 3 or 4 different directories within you project directory (use mkdir ): data/ for keeping the raw data results/ for all the outputs from the multiple analyses that you will perform docs/ for all the notes written about the analyses carried out (ex: history > 20221114.logs for the commands executed today) scripts/ for all the scripts that you will use to produce the results Note You should always have the raw data in (at least) one place and not modify them More about data structure and metadata # direct link to the tutorial used fo the lesson: Shell genomics: project organisation good practice for the structure of data and metadata of a genomics project: Organisation of genomics project Some extra material: Spreadsheet ecology lesson Exercise # This exercise combines the knowledge you have acquired during the unix and project organisation lessons. You have designed an experiment where you are studying the species and weight of animals caught in plots in a study area. Data was collected by a third party a deposited in figshare , a public database. Our goals are to download and exploring the data, while keeping an organised project directory Set up # First we go to our working directory for this training and create a project directory cd ~/bioinfo_training mkdir animals cd animals As we saw during the project organization tutorial, it is good practice to separate data, results and scripts. Let us create those three directories mkdir data results scripts Downloading the data # First we go to our data directory cd data then we download our data file and give it a more appropriate name wget https://ndownloader.figshare.com/files/2292169 mv 2292169 data_joined.csv Since we'll never modify our raw data file (or at least we do not want to! ) it is safer to remove the writing permissions chmod -w data_joined.csv ls -l Note what if my data is really big? Usually when you download data that is several gigabytes large, they will usually be compressed. You learnt about compression during the installing software lesson. Let us look at the first few lines of our file: cd .. head data/data_joined.csv Our data file is a .csv file, that is a file where fields are separated by commas , . Each row represent an animal that was caught in a plot, and each column contains information about that animal. Question How many animals do we have? wc -l data/data_joined.csv # 34787 data/data_joined.csv It seems that our dataset contains 34787 lines. Since each line is an animals, we caught a grand total of 34787 animals over the course of our study. Our first analysis script # we saw when we did the head command that all 10 first plots captured rodents. Question Is rodent the only taxon that we have captured? In our csv file, we can see that \"taxa\" is the 12th column. We can print only that column using the cut command cut -d ',' -f 12 data/data_joined.csv | head We still pipe in in head because we do not want to print 34787 line to our screen. Additionally head makes us notice that we still have the column header printed out cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | uniq -c But while uniq is supposed to count all occurrence of a word, it only count similar adjacent occurrences. Before counting, we need to sort our input: cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | sort | uniq -c We see that although we caught a vast majority of rodents, we also caught reptiles, birds and rabbits! Now that we have a working one-liner, let us put it into a script nano scripts/taxa_count.sh and write # script that prints the count of species for csv files cut -d ',' -f 12 \"$1\" | tail -n +2 | sort | uniq -c Saving the result # bash scripts/taxa_count.sh data/data_joined.csv > results/taxa_count.txt cat results/taxa_count.txt Improving our script # We would also like to know the distribution of the numbers of animals caught in plots each year. The year is the 4th column in our dataset and our script, in its current state, always selects the 12th columns of a file. We can change our script to make it flexible so that the user can chose which columns they wishes to work on. nano scripts/taxa_count.sh # script that prints the count of occurrence in one column for csv files cut -d ',' -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c Now it doesn't make much sense to have it named taxa_count.sh mv scripts/taxa_count.sh scripts/column_count.sh Question which year did we catch the most animals? try to answer programmatically. Question save the sorted output to a file in the results directory. Investigating further # We'd like to refine our animal count and knowing how many animals of each taxon were captured every year we can use cut on several columns like this: cut -d ',' -f 4,12 \"data/data_joined.csv\" | tail -n +2 | sort | uniq -c Now that we are happy with our one-liner, let us save it in a script: nano scripts/taxa_per_year.sh then save the output to results/taxa_per_year.txt bash scripts/taxa_per_year.sh > results/taxa_per_year.txt Question Which year was the first reptile captured? The next step would be to refine our analysis by year. We will save one individual output for each year count The seq command # To perform what we want to do, we need to be able to loop over the years. The seq command can help us with that. First we try seq 1 10 then seq 1997 2002 and what about the span of years we are interested in? seq 1977 2002 Great! So now does it work with a for loop? for year in $(seq 1977 2002) do echo $year done It does! Before doing our analysis on each year, we still have to figure out how to do it on one year. grep 1998 results/taxa_per_year.txt \"Grepping\" the year seems to work. Now we need to save it into a file containing the year First let's create a directory where to store our results mkdir results/years and we try to redirect our yearly count into a file grep 1998 results/taxa_per_year.txt > results/years/1998-count.txt bash cat results/years/1998-count.txt It seems to have worked. Now with the loop for year in $(seq 1977 2002) do grep $year results/taxa_per_year.txt > results/years/$year-count.txt done ls results/years Question Put your loop in a script","title":"Project organisation"},{"location":"project_organisation/#project-organization-and-management","text":"Most of the the project organization material can be found at https://software-carpentry.org and http://www.datacarpentry.org Many thanks to them for existing!","title":"Project organization and management"},{"location":"project_organisation/#structure-or-architecture-of-a-data-science-project","text":"Some good practice when you will organise your project directory on the server, on the cloud or any other machine where you will compute: Create 3 or 4 different directories within you project directory (use mkdir ): data/ for keeping the raw data results/ for all the outputs from the multiple analyses that you will perform docs/ for all the notes written about the analyses carried out (ex: history > 20221114.logs for the commands executed today) scripts/ for all the scripts that you will use to produce the results Note You should always have the raw data in (at least) one place and not modify them","title":"Structure or architecture of a data science project"},{"location":"project_organisation/#more-about-data-structure-and-metadata","text":"direct link to the tutorial used fo the lesson: Shell genomics: project organisation good practice for the structure of data and metadata of a genomics project: Organisation of genomics project Some extra material: Spreadsheet ecology lesson","title":"More about data structure and metadata"},{"location":"project_organisation/#exercise","text":"This exercise combines the knowledge you have acquired during the unix and project organisation lessons. You have designed an experiment where you are studying the species and weight of animals caught in plots in a study area. Data was collected by a third party a deposited in figshare , a public database. Our goals are to download and exploring the data, while keeping an organised project directory","title":"Exercise"},{"location":"project_organisation/#set-up","text":"First we go to our working directory for this training and create a project directory cd ~/bioinfo_training mkdir animals cd animals As we saw during the project organization tutorial, it is good practice to separate data, results and scripts. Let us create those three directories mkdir data results scripts","title":"Set up"},{"location":"project_organisation/#downloading-the-data","text":"First we go to our data directory cd data then we download our data file and give it a more appropriate name wget https://ndownloader.figshare.com/files/2292169 mv 2292169 data_joined.csv Since we'll never modify our raw data file (or at least we do not want to! ) it is safer to remove the writing permissions chmod -w data_joined.csv ls -l Note what if my data is really big? Usually when you download data that is several gigabytes large, they will usually be compressed. You learnt about compression during the installing software lesson. Let us look at the first few lines of our file: cd .. head data/data_joined.csv Our data file is a .csv file, that is a file where fields are separated by commas , . Each row represent an animal that was caught in a plot, and each column contains information about that animal. Question How many animals do we have? wc -l data/data_joined.csv # 34787 data/data_joined.csv It seems that our dataset contains 34787 lines. Since each line is an animals, we caught a grand total of 34787 animals over the course of our study.","title":"Downloading the data"},{"location":"project_organisation/#our-first-analysis-script","text":"we saw when we did the head command that all 10 first plots captured rodents. Question Is rodent the only taxon that we have captured? In our csv file, we can see that \"taxa\" is the 12th column. We can print only that column using the cut command cut -d ',' -f 12 data/data_joined.csv | head We still pipe in in head because we do not want to print 34787 line to our screen. Additionally head makes us notice that we still have the column header printed out cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | uniq -c But while uniq is supposed to count all occurrence of a word, it only count similar adjacent occurrences. Before counting, we need to sort our input: cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | sort | uniq -c We see that although we caught a vast majority of rodents, we also caught reptiles, birds and rabbits! Now that we have a working one-liner, let us put it into a script nano scripts/taxa_count.sh and write # script that prints the count of species for csv files cut -d ',' -f 12 \"$1\" | tail -n +2 | sort | uniq -c","title":"Our first analysis script"},{"location":"project_organisation/#saving-the-result","text":"bash scripts/taxa_count.sh data/data_joined.csv > results/taxa_count.txt cat results/taxa_count.txt","title":"Saving the result"},{"location":"project_organisation/#improving-our-script","text":"We would also like to know the distribution of the numbers of animals caught in plots each year. The year is the 4th column in our dataset and our script, in its current state, always selects the 12th columns of a file. We can change our script to make it flexible so that the user can chose which columns they wishes to work on. nano scripts/taxa_count.sh # script that prints the count of occurrence in one column for csv files cut -d ',' -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c Now it doesn't make much sense to have it named taxa_count.sh mv scripts/taxa_count.sh scripts/column_count.sh Question which year did we catch the most animals? try to answer programmatically. Question save the sorted output to a file in the results directory.","title":"Improving our script"},{"location":"project_organisation/#investigating-further","text":"We'd like to refine our animal count and knowing how many animals of each taxon were captured every year we can use cut on several columns like this: cut -d ',' -f 4,12 \"data/data_joined.csv\" | tail -n +2 | sort | uniq -c Now that we are happy with our one-liner, let us save it in a script: nano scripts/taxa_per_year.sh then save the output to results/taxa_per_year.txt bash scripts/taxa_per_year.sh > results/taxa_per_year.txt Question Which year was the first reptile captured? The next step would be to refine our analysis by year. We will save one individual output for each year count","title":"Investigating further"},{"location":"project_organisation/#the-seq-command","text":"To perform what we want to do, we need to be able to loop over the years. The seq command can help us with that. First we try seq 1 10 then seq 1997 2002 and what about the span of years we are interested in? seq 1977 2002 Great! So now does it work with a for loop? for year in $(seq 1977 2002) do echo $year done It does! Before doing our analysis on each year, we still have to figure out how to do it on one year. grep 1998 results/taxa_per_year.txt \"Grepping\" the year seems to work. Now we need to save it into a file containing the year First let's create a directory where to store our results mkdir results/years and we try to redirect our yearly count into a file grep 1998 results/taxa_per_year.txt > results/years/1998-count.txt bash cat results/years/1998-count.txt It seems to have worked. Now with the loop for year in $(seq 1977 2002) do grep $year results/taxa_per_year.txt > results/years/$year-count.txt done ls results/years Question Put your loop in a script","title":"The seq command"},{"location":"qc_sr/","text":"Quality Control and Trimming # Lecture # Practical # In this practical you will learn to import, view and check the quality of raw high throughput sequencing data. The first dataset you will be working with is from an Illumina dataset. The sequenced organism is a Klebsiella pneumoniae bacterium, a potentially fatal pathogen. ... Here is the article of the study. Prepare our computing environment # We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --pty bash -i You are now on a computing node, with computing core reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit Downloading the data # The raw data were deposited at the European Nucleotide Archive, under the accession number PRJEB45084. You could go to the ENA website and search for the run with the accession PRJEB45084. First create a data/ directory in your home folder for the training mkdir data now let's download the reads data: cd data wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR595/ERR5951443/K2_Illu_R1.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR595/ERR5951443/K2_Illu_R2.fastq.gz Let\u2019s make sure we downloaded all of our data using md5sum. md5sum K2_Illu_R1.fastq.gz K2_Illu_R2.fastq.gz you should see this e4f7a5de95bbb5d5d58c142afa9bc838 K2_Illu_R1.fastq.gz a5659b9b4646d15622a1a7a5c41fb22c K2_Illu_R2.fastq.gz and now look at the file names and their size ls -l total 172M -rw-r--r-- 1 hayer 83M Oct 25 16:00 K2_Illu_R1.fastq.gz -rw-r--r-- 1 hayer 89M Oct 25 16:00 K2_Illu_R2.fastq.gz One last thing before we get to the quality control: those files are writeable. By default, UNIX makes things writeable by the file owner. This poses an issue with creating typos or errors in raw data. We fix that before going further chmod u-w *.fastq.gz Question Which difference do you see when you type ls -s ? Working Directory # First we make a working directory ( results ): a directory where we can play around with a copy of the data without messing with the original cd .. mkdir results cd results Now we make a link of the data in our results directory ln -s ../data/*.fastq.gz . The files that we've downloaded are FASTQ files. Take a look at one of them with zless K2_Illu_R1.fastq.gz Tip Use the spacebar to scroll down, and type q to exit less You can read more on the FASTQ format in the File Formats lesson. Question Where does the filename come from? Question Why are there R1 and R2 in the file names? FastQC # To check the quality of the sequence data we will use a tool called FastQC. FastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation. It is available here . However, FastQC is also available as a command line utility on the training server you are using. To run FastQC on our two files module load bioinfo/FastQC/0.11.9 fastqc K2_Illu_R1.fastq.gz K2_Illu_R2.fastq.gz and look what FastQC has produced. If you want to avoid some scp or rsync commands to the NAS and to your computer, you can find the html reports for R1 and R2 ls *fastqc* For each file, FastQC has produced both a .zip archive containing all the plots, and a html report. Download and open the html files with your favourite web browser. Question What should you pay attention to in the FastQC report? Question Which file is of better quality? Pay special attention to the per base sequence quality and sequence length distribution. Explanations for the various quality modules can be found here . Also, have a look at examples of a good and a bad illumina read set for comparison. You will note that the reads in your uploaded dataset have fairly high quality (>20), which drops a bit towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads. Fastp # Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well. Incorrectly called bases in both regions can negatively impact assemblies, mapping, and downstream bioinformatics analyses. Now we will do some trimming of the reads, for keeping only high quality bases and removing potential sequencing adapters. Fastp is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It can also discard reads based upon a length threshold. module load bioinfo/fastp/0.20.1 fastp -i K2_Illu_R1.fastq.gz -I K2_Illu_R2.fastq.gz -o K2_Illu_R1_trimmed.fastq -O K2_Illu_R2_trimmed.fastq \\ --detect_adapter_for_pe --qualified_quality_phred 20 --cut_front --cut_tail --cut_mean_quality 20 \\ --html \"K2_Illu_fastp_report.html\" Question What does fastp says about what it has done to the dataset? What are the differences before and after the trimming? Tip If you want to avoid some scp or rsync commands to the NAS and to your computer, you can find the fastp report here MultiQC # MultiQC is a tool that aggregates results from several popular QC bioinformatics software into one html report. Let's run MultiQC in our current directory module load bioinfo/multiqc/1.9 multiqc . You can download the report or view it by clicking on the link below multiqc report Question What did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?","title":"Short Reads Quality Control and Trimming"},{"location":"qc_sr/#quality-control-and-trimming","text":"","title":"Quality Control and Trimming"},{"location":"qc_sr/#lecture","text":"","title":"Lecture"},{"location":"qc_sr/#practical","text":"In this practical you will learn to import, view and check the quality of raw high throughput sequencing data. The first dataset you will be working with is from an Illumina dataset. The sequenced organism is a Klebsiella pneumoniae bacterium, a potentially fatal pathogen. ... Here is the article of the study.","title":"Practical"},{"location":"qc_sr/#prepare-our-computing-environment","text":"We will first run the appropriate srun command to book the computing cores (cpus) on the cluster. Tip You need to ask the teacher which partition to use ! srun -p SELECTED_PARTITION --pty bash -i You are now on a computing node, with computing core reserved for you. That way, you can run commands interactively. If you want to exit the srun interactive mode, press CTRL+D or type exit","title":"Prepare our computing environment"},{"location":"qc_sr/#downloading-the-data","text":"The raw data were deposited at the European Nucleotide Archive, under the accession number PRJEB45084. You could go to the ENA website and search for the run with the accession PRJEB45084. First create a data/ directory in your home folder for the training mkdir data now let's download the reads data: cd data wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR595/ERR5951443/K2_Illu_R1.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR595/ERR5951443/K2_Illu_R2.fastq.gz Let\u2019s make sure we downloaded all of our data using md5sum. md5sum K2_Illu_R1.fastq.gz K2_Illu_R2.fastq.gz you should see this e4f7a5de95bbb5d5d58c142afa9bc838 K2_Illu_R1.fastq.gz a5659b9b4646d15622a1a7a5c41fb22c K2_Illu_R2.fastq.gz and now look at the file names and their size ls -l total 172M -rw-r--r-- 1 hayer 83M Oct 25 16:00 K2_Illu_R1.fastq.gz -rw-r--r-- 1 hayer 89M Oct 25 16:00 K2_Illu_R2.fastq.gz One last thing before we get to the quality control: those files are writeable. By default, UNIX makes things writeable by the file owner. This poses an issue with creating typos or errors in raw data. We fix that before going further chmod u-w *.fastq.gz Question Which difference do you see when you type ls -s ?","title":"Downloading the data"},{"location":"qc_sr/#working-directory","text":"First we make a working directory ( results ): a directory where we can play around with a copy of the data without messing with the original cd .. mkdir results cd results Now we make a link of the data in our results directory ln -s ../data/*.fastq.gz . The files that we've downloaded are FASTQ files. Take a look at one of them with zless K2_Illu_R1.fastq.gz Tip Use the spacebar to scroll down, and type q to exit less You can read more on the FASTQ format in the File Formats lesson. Question Where does the filename come from? Question Why are there R1 and R2 in the file names?","title":"Working Directory"},{"location":"qc_sr/#fastqc","text":"To check the quality of the sequence data we will use a tool called FastQC. FastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation. It is available here . However, FastQC is also available as a command line utility on the training server you are using. To run FastQC on our two files module load bioinfo/FastQC/0.11.9 fastqc K2_Illu_R1.fastq.gz K2_Illu_R2.fastq.gz and look what FastQC has produced. If you want to avoid some scp or rsync commands to the NAS and to your computer, you can find the html reports for R1 and R2 ls *fastqc* For each file, FastQC has produced both a .zip archive containing all the plots, and a html report. Download and open the html files with your favourite web browser. Question What should you pay attention to in the FastQC report? Question Which file is of better quality? Pay special attention to the per base sequence quality and sequence length distribution. Explanations for the various quality modules can be found here . Also, have a look at examples of a good and a bad illumina read set for comparison. You will note that the reads in your uploaded dataset have fairly high quality (>20), which drops a bit towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads.","title":"FastQC"},{"location":"qc_sr/#fastp","text":"Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well. Incorrectly called bases in both regions can negatively impact assemblies, mapping, and downstream bioinformatics analyses. Now we will do some trimming of the reads, for keeping only high quality bases and removing potential sequencing adapters. Fastp is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It can also discard reads based upon a length threshold. module load bioinfo/fastp/0.20.1 fastp -i K2_Illu_R1.fastq.gz -I K2_Illu_R2.fastq.gz -o K2_Illu_R1_trimmed.fastq -O K2_Illu_R2_trimmed.fastq \\ --detect_adapter_for_pe --qualified_quality_phred 20 --cut_front --cut_tail --cut_mean_quality 20 \\ --html \"K2_Illu_fastp_report.html\" Question What does fastp says about what it has done to the dataset? What are the differences before and after the trimming? Tip If you want to avoid some scp or rsync commands to the NAS and to your computer, you can find the fastp report here","title":"Fastp"},{"location":"qc_sr/#multiqc","text":"MultiQC is a tool that aggregates results from several popular QC bioinformatics software into one html report. Let's run MultiQC in our current directory module load bioinfo/multiqc/1.9 multiqc . You can download the report or view it by clicking on the link below multiqc report Question What did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?","title":"MultiQC"},{"location":"unix/","text":"Introduction to Unix # Most of the introduction to Unix material can be found at https://software-carpentry.org Many thanks to them for existing! Useful resources # Below you will find links to various useful resources for learning or using the UNIX shell. Link to the course material from software carpentry reference of concepts and commands seen during the lesson shell commands explained - a website that shows the help text of any command awesome bash - an awesome list of resources about the bash shell tldp - the linux documentation project (the books can be hard to digest but are very thorough)","title":"The Unix system"},{"location":"unix/#introduction-to-unix","text":"Most of the introduction to Unix material can be found at https://software-carpentry.org Many thanks to them for existing!","title":"Introduction to Unix"},{"location":"unix/#useful-resources","text":"Below you will find links to various useful resources for learning or using the UNIX shell. Link to the course material from software carpentry reference of concepts and commands seen during the lesson shell commands explained - a website that shows the help text of any command awesome bash - an awesome list of resources about the bash shell tldp - the linux documentation project (the books can be hard to digest but are very thorough)","title":"Useful resources"},{"location":"usage/","text":"Welcome to the bioinformatics training for Bacterial Genomics and AMR # You will find on the left menu several tutorials, from basics unix manipulations to more complex HTS data analyses. Thanks and contributors # The tutorials are developed by Juliette Hayer and Lokman Galal (Institut de Recherche pour le D\u00e9veloppement) Many of them are widely inspired from the teaching materials of Hadrien Gourl\u00e9 ( https://github.com/HadrienG ). Thanks to him :-) We created some of the tutorials together during our time at the Swedish University of Agricultural Sciences (SLU). Thanks to SLU, to the SLU Global bioinformatics Centre (SGBC) and to SLUBI ( SLU Bioinformatics Infrastructure )","title":"Main page"},{"location":"usage/#welcome-to-the-bioinformatics-training-for-bacterial-genomics-and-amr","text":"You will find on the left menu several tutorials, from basics unix manipulations to more complex HTS data analyses.","title":"Welcome to the bioinformatics training for Bacterial Genomics and AMR"},{"location":"usage/#thanks-and-contributors","text":"The tutorials are developed by Juliette Hayer and Lokman Galal (Institut de Recherche pour le D\u00e9veloppement) Many of them are widely inspired from the teaching materials of Hadrien Gourl\u00e9 ( https://github.com/HadrienG ). Thanks to him :-) We created some of the tutorials together during our time at the Swedish University of Agricultural Sciences (SLU). Thanks to SLU, to the SLU Global bioinformatics Centre (SGBC) and to SLUBI ( SLU Bioinformatics Infrastructure )","title":"Thanks and contributors"},{"location":"variant_calling/","text":"VARIANT CALLING # Variant calling workflow tutorial Tutorial for automating variant calling workflow","title":"Mapping and variant calling"},{"location":"variant_calling/#variant-calling","text":"Variant calling workflow tutorial Tutorial for automating variant calling workflow","title":"VARIANT CALLING"}]}